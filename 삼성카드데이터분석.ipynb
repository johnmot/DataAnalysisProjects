{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YM</th>\n",
       "      <th>Category</th>\n",
       "      <th>성별구분</th>\n",
       "      <th>연령대</th>\n",
       "      <th>기혼스코어</th>\n",
       "      <th>유아자녀스코어</th>\n",
       "      <th>초등학생자녀스코어</th>\n",
       "      <th>중고생자녀스코어</th>\n",
       "      <th>대학생자녀스코어</th>\n",
       "      <th>전업주부스코어</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>202005</td>\n",
       "      <td>할인점</td>\n",
       "      <td>0</td>\n",
       "      <td>F</td>\n",
       "      <td>high</td>\n",
       "      <td>low</td>\n",
       "      <td>high</td>\n",
       "      <td>mid</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>202005</td>\n",
       "      <td>취미</td>\n",
       "      <td>0</td>\n",
       "      <td>B</td>\n",
       "      <td>high</td>\n",
       "      <td>low</td>\n",
       "      <td>mid</td>\n",
       "      <td>mid</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>202005</td>\n",
       "      <td>오픈마켓/소셜</td>\n",
       "      <td>1</td>\n",
       "      <td>D</td>\n",
       "      <td>mid</td>\n",
       "      <td>mid</td>\n",
       "      <td>mid</td>\n",
       "      <td>mid</td>\n",
       "      <td>low</td>\n",
       "      <td>mid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>202005</td>\n",
       "      <td>뷰티</td>\n",
       "      <td>0</td>\n",
       "      <td>D</td>\n",
       "      <td>mid</td>\n",
       "      <td>mid</td>\n",
       "      <td>mid</td>\n",
       "      <td>mid</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>202005</td>\n",
       "      <td>오픈마켓/소셜</td>\n",
       "      <td>0</td>\n",
       "      <td>G</td>\n",
       "      <td>high</td>\n",
       "      <td>low</td>\n",
       "      <td>mid</td>\n",
       "      <td>mid</td>\n",
       "      <td>mid</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452033</th>\n",
       "      <td>201904</td>\n",
       "      <td>전문몰</td>\n",
       "      <td>0</td>\n",
       "      <td>E</td>\n",
       "      <td>low</td>\n",
       "      <td>mid</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452034</th>\n",
       "      <td>201904</td>\n",
       "      <td>할인점</td>\n",
       "      <td>0</td>\n",
       "      <td>H</td>\n",
       "      <td>high</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452035</th>\n",
       "      <td>201904</td>\n",
       "      <td>할인점</td>\n",
       "      <td>1</td>\n",
       "      <td>H</td>\n",
       "      <td>high</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452036</th>\n",
       "      <td>201904</td>\n",
       "      <td>할인점</td>\n",
       "      <td>0</td>\n",
       "      <td>H</td>\n",
       "      <td>high</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452037</th>\n",
       "      <td>201904</td>\n",
       "      <td>할인점</td>\n",
       "      <td>1</td>\n",
       "      <td>B</td>\n",
       "      <td>high</td>\n",
       "      <td>low</td>\n",
       "      <td>mid</td>\n",
       "      <td>mid</td>\n",
       "      <td>low</td>\n",
       "      <td>mid</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>452038 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            YM Category  성별구분 연령대 기혼스코어 유아자녀스코어 초등학생자녀스코어 중고생자녀스코어 대학생자녀스코어  \\\n",
       "0       202005      할인점     0   F  high     low      high      mid      low   \n",
       "1       202005       취미     0   B  high     low       mid      mid      low   \n",
       "2       202005  오픈마켓/소셜     1   D   mid     mid       mid      mid      low   \n",
       "3       202005       뷰티     0   D   mid     mid       mid      mid      low   \n",
       "4       202005  오픈마켓/소셜     0   G  high     low       mid      mid      mid   \n",
       "...        ...      ...   ...  ..   ...     ...       ...      ...      ...   \n",
       "452033  201904      전문몰     0   E   low     mid       low      low      low   \n",
       "452034  201904      할인점     0   H  high     low       low      low      low   \n",
       "452035  201904      할인점     1   H  high     low       low      low      low   \n",
       "452036  201904      할인점     0   H  high     low       low      low      low   \n",
       "452037  201904      할인점     1   B  high     low       mid      mid      low   \n",
       "\n",
       "       전업주부스코어  \n",
       "0          low  \n",
       "1          low  \n",
       "2          mid  \n",
       "3          low  \n",
       "4          low  \n",
       "...        ...  \n",
       "452033     low  \n",
       "452034     low  \n",
       "452035     low  \n",
       "452036     low  \n",
       "452037     mid  \n",
       "\n",
       "[452038 rows x 10 columns]"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#print(os.getcwd())\n",
    "category = pd.read_csv(\"/Users/booknfoto/Desktop/dataset/[Track1_데이터1] mrc_info.csv\", encoding=\"euc-kr\")\n",
    "train = pd.read_csv(\"/Users/booknfoto/Desktop/dataset/[Track1_데이터2] samp_train.csv\", encoding=\"euc-kr\")\n",
    "samp = pd.read_csv(\"/Users/booknfoto/Desktop/dataset/[Track1_데이터3] samp_cst_feat.csv\", encoding=\"euc-kr\")\n",
    "trend = pd.read_csv(\"/Users/booknfoto/Desktop/dataset/[Track2_데이터1] trend_w_demo.csv\", encoding=\"euc-kr\")\n",
    "trend\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cst_id_di</th>\n",
       "      <th>VAR002</th>\n",
       "      <th>VAR003</th>\n",
       "      <th>VAR004</th>\n",
       "      <th>VAR005</th>\n",
       "      <th>VAR006</th>\n",
       "      <th>VAR007</th>\n",
       "      <th>VAR008</th>\n",
       "      <th>VAR009</th>\n",
       "      <th>VAR010</th>\n",
       "      <th>...</th>\n",
       "      <th>VAR219</th>\n",
       "      <th>VAR220</th>\n",
       "      <th>VAR221</th>\n",
       "      <th>VAR222</th>\n",
       "      <th>VAR223</th>\n",
       "      <th>VAR224</th>\n",
       "      <th>VAR225</th>\n",
       "      <th>VAR226</th>\n",
       "      <th>VAR227</th>\n",
       "      <th>MRC_ID_DI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>90000000089</td>\n",
       "      <td>-0.06610</td>\n",
       "      <td>0.5280</td>\n",
       "      <td>-0.13607</td>\n",
       "      <td>0.10945</td>\n",
       "      <td>0.06557</td>\n",
       "      <td>0</td>\n",
       "      <td>0.7702</td>\n",
       "      <td>-0.18965</td>\n",
       "      <td>0.1981</td>\n",
       "      <td>...</td>\n",
       "      <td>0.19113</td>\n",
       "      <td>0.05449</td>\n",
       "      <td>0.09471</td>\n",
       "      <td>0.27091</td>\n",
       "      <td>0.01931</td>\n",
       "      <td>0.02938</td>\n",
       "      <td>0.17105</td>\n",
       "      <td>0.12537</td>\n",
       "      <td>0.22197</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90000000176</td>\n",
       "      <td>-0.09537</td>\n",
       "      <td>0.1347</td>\n",
       "      <td>-0.13541</td>\n",
       "      <td>0.17331</td>\n",
       "      <td>-0.19657</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0616</td>\n",
       "      <td>-0.23104</td>\n",
       "      <td>0.4940</td>\n",
       "      <td>...</td>\n",
       "      <td>0.19437</td>\n",
       "      <td>0.06538</td>\n",
       "      <td>0.16309</td>\n",
       "      <td>0.30207</td>\n",
       "      <td>0.06053</td>\n",
       "      <td>-0.01107</td>\n",
       "      <td>0.12413</td>\n",
       "      <td>0.29702</td>\n",
       "      <td>-0.31717</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90000000210</td>\n",
       "      <td>-0.01048</td>\n",
       "      <td>0.8360</td>\n",
       "      <td>0.37797</td>\n",
       "      <td>-0.10970</td>\n",
       "      <td>0.52032</td>\n",
       "      <td>1</td>\n",
       "      <td>0.3257</td>\n",
       "      <td>0.32632</td>\n",
       "      <td>0.7343</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.52084</td>\n",
       "      <td>-0.18568</td>\n",
       "      <td>-0.09755</td>\n",
       "      <td>-0.56565</td>\n",
       "      <td>-0.17840</td>\n",
       "      <td>-0.06314</td>\n",
       "      <td>-0.17111</td>\n",
       "      <td>-0.32239</td>\n",
       "      <td>0.33962</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 228 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     cst_id_di   VAR002  VAR003   VAR004   VAR005   VAR006  VAR007  VAR008  \\\n",
       "0  90000000089 -0.06610  0.5280 -0.13607  0.10945  0.06557       0  0.7702   \n",
       "1  90000000176 -0.09537  0.1347 -0.13541  0.17331 -0.19657       0  0.0616   \n",
       "2  90000000210 -0.01048  0.8360  0.37797 -0.10970  0.52032       1  0.3257   \n",
       "\n",
       "    VAR009  VAR010  ...   VAR219   VAR220   VAR221   VAR222   VAR223   VAR224  \\\n",
       "0 -0.18965  0.1981  ...  0.19113  0.05449  0.09471  0.27091  0.01931  0.02938   \n",
       "1 -0.23104  0.4940  ...  0.19437  0.06538  0.16309  0.30207  0.06053 -0.01107   \n",
       "2  0.32632  0.7343  ... -0.52084 -0.18568 -0.09755 -0.56565 -0.17840 -0.06314   \n",
       "\n",
       "    VAR225   VAR226   VAR227  MRC_ID_DI  \n",
       "0  0.17105  0.12537  0.22197          0  \n",
       "1  0.12413  0.29702 -0.31717          8  \n",
       "2 -0.17111 -0.32239  0.33962          0  \n",
       "\n",
       "[3 rows x 228 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.merge(samp,train, on= \"cst_id_di\")\n",
    "#samp\n",
    "#train\n",
    "df1.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 10124 entries, 0 to 10123\n",
      "Columns: 228 entries, cst_id_di to MRC_ID_DI\n",
      "dtypes: float64(198), int64(30)\n",
      "memory usage: 17.7 MB\n"
     ]
    }
   ],
   "source": [
    "#데이터 개괄 확인\n",
    "df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column</th>\n",
       "      <th>count</th>\n",
       "      <th>ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [column, count, ratio]\n",
       "Index: []"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#데이터 널값 확인\n",
    "missing_df = df1.isnull().sum().reset_index()\n",
    "missing_df.columns = ['column', 'count']\n",
    "missing_df['ratio'] = missing_df['count'] /df1.shape[0]\n",
    "missing_df.loc[missing_df['ratio'] != 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     8200\n",
       "7      661\n",
       "6      554\n",
       "8      337\n",
       "5      143\n",
       "1       85\n",
       "9       59\n",
       "3       32\n",
       "10      28\n",
       "2       14\n",
       "4       11\n",
       "Name: MRC_ID_DI, dtype: int64"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#종속변수 체크\n",
    "import matplotlib.pyplot as plt\n",
    "df1['MRC_ID_DI'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    8200\n",
       "1    1924\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['label'] = [1 if s > 0 else 0 for s in df1['MRC_ID_DI']] \n",
    "df1['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MRC_ID_DI</th>\n",
       "      <th>VAR002</th>\n",
       "      <th>VAR003</th>\n",
       "      <th>VAR004</th>\n",
       "      <th>VAR005</th>\n",
       "      <th>VAR006</th>\n",
       "      <th>VAR007</th>\n",
       "      <th>VAR008</th>\n",
       "      <th>VAR009</th>\n",
       "      <th>VAR010</th>\n",
       "      <th>...</th>\n",
       "      <th>VAR220</th>\n",
       "      <th>VAR221</th>\n",
       "      <th>VAR222</th>\n",
       "      <th>VAR223</th>\n",
       "      <th>VAR224</th>\n",
       "      <th>VAR225</th>\n",
       "      <th>VAR226</th>\n",
       "      <th>VAR227</th>\n",
       "      <th>cst_id_di</th>\n",
       "      <th>var178</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.000954</td>\n",
       "      <td>0.522545</td>\n",
       "      <td>0.052767</td>\n",
       "      <td>-0.040540</td>\n",
       "      <td>0.213861</td>\n",
       "      <td>0.165854</td>\n",
       "      <td>0.502712</td>\n",
       "      <td>0.068857</td>\n",
       "      <td>0.487807</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.037691</td>\n",
       "      <td>-0.036329</td>\n",
       "      <td>-0.104435</td>\n",
       "      <td>-0.038285</td>\n",
       "      <td>-0.015856</td>\n",
       "      <td>-0.025970</td>\n",
       "      <td>-0.056926</td>\n",
       "      <td>0.144836</td>\n",
       "      <td>9.000023e+10</td>\n",
       "      <td>0.490404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.45842</td>\n",
       "      <td>-0.016376</td>\n",
       "      <td>0.320887</td>\n",
       "      <td>-0.059132</td>\n",
       "      <td>0.045345</td>\n",
       "      <td>0.008269</td>\n",
       "      <td>0.068087</td>\n",
       "      <td>0.483979</td>\n",
       "      <td>-0.078990</td>\n",
       "      <td>0.655482</td>\n",
       "      <td>...</td>\n",
       "      <td>0.051772</td>\n",
       "      <td>0.040775</td>\n",
       "      <td>0.214099</td>\n",
       "      <td>0.047283</td>\n",
       "      <td>0.002055</td>\n",
       "      <td>0.039852</td>\n",
       "      <td>0.174319</td>\n",
       "      <td>-0.043852</td>\n",
       "      <td>9.000023e+10</td>\n",
       "      <td>0.534082</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 228 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       MRC_ID_DI    VAR002    VAR003    VAR004    VAR005    VAR006    VAR007  \\\n",
       "label                                                                          \n",
       "0        0.00000 -0.000954  0.522545  0.052767 -0.040540  0.213861  0.165854   \n",
       "1        6.45842 -0.016376  0.320887 -0.059132  0.045345  0.008269  0.068087   \n",
       "\n",
       "         VAR008    VAR009    VAR010  ...    VAR220    VAR221    VAR222  \\\n",
       "label                                ...                                 \n",
       "0      0.502712  0.068857  0.487807  ... -0.037691 -0.036329 -0.104435   \n",
       "1      0.483979 -0.078990  0.655482  ...  0.051772  0.040775  0.214099   \n",
       "\n",
       "         VAR223    VAR224    VAR225    VAR226    VAR227     cst_id_di  \\\n",
       "label                                                                   \n",
       "0     -0.038285 -0.015856 -0.025970 -0.056926  0.144836  9.000023e+10   \n",
       "1      0.047283  0.002055  0.039852  0.174319 -0.043852  9.000023e+10   \n",
       "\n",
       "         var178  \n",
       "label            \n",
       "0      0.490404  \n",
       "1      0.534082  \n",
       "\n",
       "[2 rows x 228 columns]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.pivot_table(df1, index = ['label'],aggfunc = 'mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MRC_ID_DI</th>\n",
       "      <th>VAR002</th>\n",
       "      <th>VAR003</th>\n",
       "      <th>VAR004</th>\n",
       "      <th>VAR005</th>\n",
       "      <th>VAR006</th>\n",
       "      <th>VAR007</th>\n",
       "      <th>VAR008</th>\n",
       "      <th>VAR009</th>\n",
       "      <th>VAR010</th>\n",
       "      <th>...</th>\n",
       "      <th>VAR220</th>\n",
       "      <th>VAR221</th>\n",
       "      <th>VAR222</th>\n",
       "      <th>VAR223</th>\n",
       "      <th>VAR224</th>\n",
       "      <th>VAR225</th>\n",
       "      <th>VAR226</th>\n",
       "      <th>VAR227</th>\n",
       "      <th>cst_id_di</th>\n",
       "      <th>var178</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-7.82365</td>\n",
       "      <td>4284.8718</td>\n",
       "      <td>432.68931</td>\n",
       "      <td>-332.42794</td>\n",
       "      <td>1753.66341</td>\n",
       "      <td>1360</td>\n",
       "      <td>4122.2382</td>\n",
       "      <td>564.62460</td>\n",
       "      <td>4000.0142</td>\n",
       "      <td>...</td>\n",
       "      <td>-309.06262</td>\n",
       "      <td>-297.89713</td>\n",
       "      <td>-856.37101</td>\n",
       "      <td>-313.93405</td>\n",
       "      <td>-130.01716</td>\n",
       "      <td>-212.95028</td>\n",
       "      <td>-466.79173</td>\n",
       "      <td>1187.65282</td>\n",
       "      <td>738001873446410</td>\n",
       "      <td>4021.3117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12426</td>\n",
       "      <td>-31.50699</td>\n",
       "      <td>617.3857</td>\n",
       "      <td>-113.77002</td>\n",
       "      <td>87.24422</td>\n",
       "      <td>15.91023</td>\n",
       "      <td>131</td>\n",
       "      <td>931.1759</td>\n",
       "      <td>-151.97743</td>\n",
       "      <td>1261.1477</td>\n",
       "      <td>...</td>\n",
       "      <td>99.60911</td>\n",
       "      <td>78.45189</td>\n",
       "      <td>411.92685</td>\n",
       "      <td>90.97260</td>\n",
       "      <td>3.95300</td>\n",
       "      <td>76.67603</td>\n",
       "      <td>335.38982</td>\n",
       "      <td>-84.37211</td>\n",
       "      <td>173160441532680</td>\n",
       "      <td>1027.5735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 228 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       MRC_ID_DI    VAR002     VAR003     VAR004     VAR005      VAR006  \\\n",
       "label                                                                     \n",
       "0              0  -7.82365  4284.8718  432.68931 -332.42794  1753.66341   \n",
       "1          12426 -31.50699   617.3857 -113.77002   87.24422    15.91023   \n",
       "\n",
       "       VAR007     VAR008     VAR009     VAR010  ...     VAR220     VAR221  \\\n",
       "label                                           ...                         \n",
       "0        1360  4122.2382  564.62460  4000.0142  ... -309.06262 -297.89713   \n",
       "1         131   931.1759 -151.97743  1261.1477  ...   99.60911   78.45189   \n",
       "\n",
       "          VAR222     VAR223     VAR224     VAR225     VAR226      VAR227  \\\n",
       "label                                                                      \n",
       "0     -856.37101 -313.93405 -130.01716 -212.95028 -466.79173  1187.65282   \n",
       "1      411.92685   90.97260    3.95300   76.67603  335.38982   -84.37211   \n",
       "\n",
       "             cst_id_di     var178  \n",
       "label                              \n",
       "0      738001873446410  4021.3117  \n",
       "1      173160441532680  1027.5735  \n",
       "\n",
       "[2 rows x 228 columns]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.pivot_table(df1, index = ['label'],aggfunc = 'sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VAR002</th>\n",
       "      <th>VAR003</th>\n",
       "      <th>VAR004</th>\n",
       "      <th>VAR005</th>\n",
       "      <th>VAR006</th>\n",
       "      <th>VAR007</th>\n",
       "      <th>VAR008</th>\n",
       "      <th>VAR009</th>\n",
       "      <th>VAR010</th>\n",
       "      <th>VAR011</th>\n",
       "      <th>...</th>\n",
       "      <th>VAR218</th>\n",
       "      <th>VAR219</th>\n",
       "      <th>VAR220</th>\n",
       "      <th>VAR221</th>\n",
       "      <th>VAR222</th>\n",
       "      <th>VAR223</th>\n",
       "      <th>VAR224</th>\n",
       "      <th>VAR225</th>\n",
       "      <th>VAR226</th>\n",
       "      <th>VAR227</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.06610</td>\n",
       "      <td>0.5280</td>\n",
       "      <td>-0.13607</td>\n",
       "      <td>0.10945</td>\n",
       "      <td>0.06557</td>\n",
       "      <td>0</td>\n",
       "      <td>0.7702</td>\n",
       "      <td>-0.18965</td>\n",
       "      <td>0.1981</td>\n",
       "      <td>0.24149</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.32829</td>\n",
       "      <td>0.19113</td>\n",
       "      <td>0.05449</td>\n",
       "      <td>0.09471</td>\n",
       "      <td>0.27091</td>\n",
       "      <td>0.01931</td>\n",
       "      <td>0.02938</td>\n",
       "      <td>0.17105</td>\n",
       "      <td>0.12537</td>\n",
       "      <td>0.22197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.09537</td>\n",
       "      <td>0.1347</td>\n",
       "      <td>-0.13541</td>\n",
       "      <td>0.17331</td>\n",
       "      <td>-0.19657</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0616</td>\n",
       "      <td>-0.23104</td>\n",
       "      <td>0.4940</td>\n",
       "      <td>-0.39476</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.23729</td>\n",
       "      <td>0.19437</td>\n",
       "      <td>0.06538</td>\n",
       "      <td>0.16309</td>\n",
       "      <td>0.30207</td>\n",
       "      <td>0.06053</td>\n",
       "      <td>-0.01107</td>\n",
       "      <td>0.12413</td>\n",
       "      <td>0.29702</td>\n",
       "      <td>-0.31717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.01048</td>\n",
       "      <td>0.8360</td>\n",
       "      <td>0.37797</td>\n",
       "      <td>-0.10970</td>\n",
       "      <td>0.52032</td>\n",
       "      <td>1</td>\n",
       "      <td>0.3257</td>\n",
       "      <td>0.32632</td>\n",
       "      <td>0.7343</td>\n",
       "      <td>0.73494</td>\n",
       "      <td>...</td>\n",
       "      <td>0.53283</td>\n",
       "      <td>-0.52084</td>\n",
       "      <td>-0.18568</td>\n",
       "      <td>-0.09755</td>\n",
       "      <td>-0.56565</td>\n",
       "      <td>-0.17840</td>\n",
       "      <td>-0.06314</td>\n",
       "      <td>-0.17111</td>\n",
       "      <td>-0.32239</td>\n",
       "      <td>0.33962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.05194</td>\n",
       "      <td>0.7505</td>\n",
       "      <td>0.04611</td>\n",
       "      <td>-0.16512</td>\n",
       "      <td>0.07413</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5322</td>\n",
       "      <td>0.26845</td>\n",
       "      <td>0.7327</td>\n",
       "      <td>0.32617</td>\n",
       "      <td>...</td>\n",
       "      <td>0.20754</td>\n",
       "      <td>-0.01934</td>\n",
       "      <td>-0.05172</td>\n",
       "      <td>-0.13245</td>\n",
       "      <td>-0.16357</td>\n",
       "      <td>-0.05697</td>\n",
       "      <td>0.01587</td>\n",
       "      <td>-0.04022</td>\n",
       "      <td>0.31213</td>\n",
       "      <td>-0.00559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.08536</td>\n",
       "      <td>0.3767</td>\n",
       "      <td>-0.12288</td>\n",
       "      <td>0.10023</td>\n",
       "      <td>-0.43414</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5468</td>\n",
       "      <td>-0.25575</td>\n",
       "      <td>0.9644</td>\n",
       "      <td>-0.52948</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.42251</td>\n",
       "      <td>0.23122</td>\n",
       "      <td>0.07913</td>\n",
       "      <td>0.09206</td>\n",
       "      <td>0.46971</td>\n",
       "      <td>0.07964</td>\n",
       "      <td>-0.04698</td>\n",
       "      <td>0.03581</td>\n",
       "      <td>0.22588</td>\n",
       "      <td>-0.34868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10119</th>\n",
       "      <td>-0.06606</td>\n",
       "      <td>0.6615</td>\n",
       "      <td>-0.09743</td>\n",
       "      <td>-0.03240</td>\n",
       "      <td>0.10111</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9722</td>\n",
       "      <td>-0.02041</td>\n",
       "      <td>0.6966</td>\n",
       "      <td>0.06993</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.19780</td>\n",
       "      <td>0.33881</td>\n",
       "      <td>-0.01692</td>\n",
       "      <td>-0.01823</td>\n",
       "      <td>0.21720</td>\n",
       "      <td>-0.08346</td>\n",
       "      <td>-0.07835</td>\n",
       "      <td>0.02321</td>\n",
       "      <td>0.32967</td>\n",
       "      <td>-0.25995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10120</th>\n",
       "      <td>-0.03031</td>\n",
       "      <td>0.0143</td>\n",
       "      <td>0.07041</td>\n",
       "      <td>-0.02519</td>\n",
       "      <td>0.58013</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0330</td>\n",
       "      <td>0.06676</td>\n",
       "      <td>0.8251</td>\n",
       "      <td>0.68992</td>\n",
       "      <td>...</td>\n",
       "      <td>0.09259</td>\n",
       "      <td>-0.19384</td>\n",
       "      <td>-0.02383</td>\n",
       "      <td>-0.02448</td>\n",
       "      <td>-0.05019</td>\n",
       "      <td>-0.02869</td>\n",
       "      <td>-0.05401</td>\n",
       "      <td>0.01670</td>\n",
       "      <td>-0.15880</td>\n",
       "      <td>0.48301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10121</th>\n",
       "      <td>-0.05351</td>\n",
       "      <td>0.3121</td>\n",
       "      <td>0.36925</td>\n",
       "      <td>-0.10039</td>\n",
       "      <td>0.51159</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2582</td>\n",
       "      <td>0.35016</td>\n",
       "      <td>0.4638</td>\n",
       "      <td>0.68799</td>\n",
       "      <td>...</td>\n",
       "      <td>0.46478</td>\n",
       "      <td>-0.45312</td>\n",
       "      <td>-0.17163</td>\n",
       "      <td>-0.08674</td>\n",
       "      <td>-0.40260</td>\n",
       "      <td>-0.15903</td>\n",
       "      <td>-0.10292</td>\n",
       "      <td>-0.11742</td>\n",
       "      <td>-0.31895</td>\n",
       "      <td>0.40357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10122</th>\n",
       "      <td>-0.00562</td>\n",
       "      <td>0.2286</td>\n",
       "      <td>0.04581</td>\n",
       "      <td>-0.05390</td>\n",
       "      <td>0.20481</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5957</td>\n",
       "      <td>0.11319</td>\n",
       "      <td>0.2527</td>\n",
       "      <td>0.42924</td>\n",
       "      <td>...</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.01754</td>\n",
       "      <td>-0.01479</td>\n",
       "      <td>-0.03898</td>\n",
       "      <td>-0.01363</td>\n",
       "      <td>0.06974</td>\n",
       "      <td>-0.03815</td>\n",
       "      <td>-0.04371</td>\n",
       "      <td>0.11433</td>\n",
       "      <td>-0.01931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10123</th>\n",
       "      <td>-0.06814</td>\n",
       "      <td>0.6968</td>\n",
       "      <td>-0.04318</td>\n",
       "      <td>0.11340</td>\n",
       "      <td>-0.08842</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1151</td>\n",
       "      <td>-0.02036</td>\n",
       "      <td>0.8465</td>\n",
       "      <td>0.13615</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00564</td>\n",
       "      <td>0.08257</td>\n",
       "      <td>0.00120</td>\n",
       "      <td>0.08881</td>\n",
       "      <td>0.01272</td>\n",
       "      <td>-0.01391</td>\n",
       "      <td>-0.05940</td>\n",
       "      <td>0.44214</td>\n",
       "      <td>0.22888</td>\n",
       "      <td>-0.09918</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10124 rows × 226 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        VAR002  VAR003   VAR004   VAR005   VAR006  VAR007  VAR008   VAR009  \\\n",
       "0     -0.06610  0.5280 -0.13607  0.10945  0.06557       0  0.7702 -0.18965   \n",
       "1     -0.09537  0.1347 -0.13541  0.17331 -0.19657       0  0.0616 -0.23104   \n",
       "2     -0.01048  0.8360  0.37797 -0.10970  0.52032       1  0.3257  0.32632   \n",
       "3      0.05194  0.7505  0.04611 -0.16512  0.07413       0  0.5322  0.26845   \n",
       "4     -0.08536  0.3767 -0.12288  0.10023 -0.43414       0  0.5468 -0.25575   \n",
       "...        ...     ...      ...      ...      ...     ...     ...      ...   \n",
       "10119 -0.06606  0.6615 -0.09743 -0.03240  0.10111       0  0.9722 -0.02041   \n",
       "10120 -0.03031  0.0143  0.07041 -0.02519  0.58013       0  0.0330  0.06676   \n",
       "10121 -0.05351  0.3121  0.36925 -0.10039  0.51159       0  0.2582  0.35016   \n",
       "10122 -0.00562  0.2286  0.04581 -0.05390  0.20481       0  0.5957  0.11319   \n",
       "10123 -0.06814  0.6968 -0.04318  0.11340 -0.08842       0  0.1151 -0.02036   \n",
       "\n",
       "       VAR010   VAR011  ...   VAR218   VAR219   VAR220   VAR221   VAR222  \\\n",
       "0      0.1981  0.24149  ... -0.32829  0.19113  0.05449  0.09471  0.27091   \n",
       "1      0.4940 -0.39476  ... -0.23729  0.19437  0.06538  0.16309  0.30207   \n",
       "2      0.7343  0.73494  ...  0.53283 -0.52084 -0.18568 -0.09755 -0.56565   \n",
       "3      0.7327  0.32617  ...  0.20754 -0.01934 -0.05172 -0.13245 -0.16357   \n",
       "4      0.9644 -0.52948  ... -0.42251  0.23122  0.07913  0.09206  0.46971   \n",
       "...       ...      ...  ...      ...      ...      ...      ...      ...   \n",
       "10119  0.6966  0.06993  ... -0.19780  0.33881 -0.01692 -0.01823  0.21720   \n",
       "10120  0.8251  0.68992  ...  0.09259 -0.19384 -0.02383 -0.02448 -0.05019   \n",
       "10121  0.4638  0.68799  ...  0.46478 -0.45312 -0.17163 -0.08674 -0.40260   \n",
       "10122  0.2527  0.42924  ...  0.10340  0.01754 -0.01479 -0.03898 -0.01363   \n",
       "10123  0.8465  0.13615  ...  0.00564  0.08257  0.00120  0.08881  0.01272   \n",
       "\n",
       "        VAR223   VAR224   VAR225   VAR226   VAR227  \n",
       "0      0.01931  0.02938  0.17105  0.12537  0.22197  \n",
       "1      0.06053 -0.01107  0.12413  0.29702 -0.31717  \n",
       "2     -0.17840 -0.06314 -0.17111 -0.32239  0.33962  \n",
       "3     -0.05697  0.01587 -0.04022  0.31213 -0.00559  \n",
       "4      0.07964 -0.04698  0.03581  0.22588 -0.34868  \n",
       "...        ...      ...      ...      ...      ...  \n",
       "10119 -0.08346 -0.07835  0.02321  0.32967 -0.25995  \n",
       "10120 -0.02869 -0.05401  0.01670 -0.15880  0.48301  \n",
       "10121 -0.15903 -0.10292 -0.11742 -0.31895  0.40357  \n",
       "10122  0.06974 -0.03815 -0.04371  0.11433 -0.01931  \n",
       "10123 -0.01391 -0.05940  0.44214  0.22888 -0.09918  \n",
       "\n",
       "[10124 rows x 226 columns]"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = df1.iloc[:,1:227]\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VAR002</th>\n",
       "      <th>VAR003</th>\n",
       "      <th>VAR004</th>\n",
       "      <th>VAR005</th>\n",
       "      <th>VAR006</th>\n",
       "      <th>VAR007</th>\n",
       "      <th>VAR008</th>\n",
       "      <th>VAR009</th>\n",
       "      <th>VAR010</th>\n",
       "      <th>VAR011</th>\n",
       "      <th>...</th>\n",
       "      <th>VAR219</th>\n",
       "      <th>VAR220</th>\n",
       "      <th>VAR221</th>\n",
       "      <th>VAR222</th>\n",
       "      <th>VAR223</th>\n",
       "      <th>VAR224</th>\n",
       "      <th>VAR225</th>\n",
       "      <th>VAR226</th>\n",
       "      <th>VAR227</th>\n",
       "      <th>var178</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8200</td>\n",
       "      <td>8200</td>\n",
       "      <td>8200</td>\n",
       "      <td>8200</td>\n",
       "      <td>8200</td>\n",
       "      <td>8200</td>\n",
       "      <td>8200</td>\n",
       "      <td>8200</td>\n",
       "      <td>8200</td>\n",
       "      <td>8200</td>\n",
       "      <td>...</td>\n",
       "      <td>8200</td>\n",
       "      <td>8200</td>\n",
       "      <td>8200</td>\n",
       "      <td>8200</td>\n",
       "      <td>8200</td>\n",
       "      <td>8200</td>\n",
       "      <td>8200</td>\n",
       "      <td>8200</td>\n",
       "      <td>8200</td>\n",
       "      <td>8200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1924</td>\n",
       "      <td>1924</td>\n",
       "      <td>1924</td>\n",
       "      <td>1924</td>\n",
       "      <td>1924</td>\n",
       "      <td>1924</td>\n",
       "      <td>1924</td>\n",
       "      <td>1924</td>\n",
       "      <td>1924</td>\n",
       "      <td>1924</td>\n",
       "      <td>...</td>\n",
       "      <td>1924</td>\n",
       "      <td>1924</td>\n",
       "      <td>1924</td>\n",
       "      <td>1924</td>\n",
       "      <td>1924</td>\n",
       "      <td>1924</td>\n",
       "      <td>1924</td>\n",
       "      <td>1924</td>\n",
       "      <td>1924</td>\n",
       "      <td>1924</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 226 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       VAR002  VAR003  VAR004  VAR005  VAR006  VAR007  VAR008  VAR009  VAR010  \\\n",
       "label                                                                           \n",
       "0        8200    8200    8200    8200    8200    8200    8200    8200    8200   \n",
       "1        1924    1924    1924    1924    1924    1924    1924    1924    1924   \n",
       "\n",
       "       VAR011  ...  VAR219  VAR220  VAR221  VAR222  VAR223  VAR224  VAR225  \\\n",
       "label          ...                                                           \n",
       "0        8200  ...    8200    8200    8200    8200    8200    8200    8200   \n",
       "1        1924  ...    1924    1924    1924    1924    1924    1924    1924   \n",
       "\n",
       "       VAR226  VAR227  var178  \n",
       "label                          \n",
       "0        8200    8200    8200  \n",
       "1        1924    1924    1924  \n",
       "\n",
       "[2 rows x 226 columns]"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from imblearn.under_sampling import *\n",
    "df3 = df1.iloc[:,1:227]\n",
    "pd.pivot_table(df3, index = df1['label'], aggfunc = 'count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VAR002</th>\n",
       "      <th>VAR003</th>\n",
       "      <th>VAR004</th>\n",
       "      <th>VAR005</th>\n",
       "      <th>VAR006</th>\n",
       "      <th>VAR007</th>\n",
       "      <th>VAR008</th>\n",
       "      <th>VAR009</th>\n",
       "      <th>VAR010</th>\n",
       "      <th>VAR011</th>\n",
       "      <th>...</th>\n",
       "      <th>VAR219</th>\n",
       "      <th>VAR220</th>\n",
       "      <th>VAR221</th>\n",
       "      <th>VAR222</th>\n",
       "      <th>VAR223</th>\n",
       "      <th>VAR224</th>\n",
       "      <th>VAR225</th>\n",
       "      <th>VAR226</th>\n",
       "      <th>VAR227</th>\n",
       "      <th>var178</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1924</td>\n",
       "      <td>1924</td>\n",
       "      <td>1924</td>\n",
       "      <td>1924</td>\n",
       "      <td>1924</td>\n",
       "      <td>1924</td>\n",
       "      <td>1924</td>\n",
       "      <td>1924</td>\n",
       "      <td>1924</td>\n",
       "      <td>1924</td>\n",
       "      <td>...</td>\n",
       "      <td>1924</td>\n",
       "      <td>1924</td>\n",
       "      <td>1924</td>\n",
       "      <td>1924</td>\n",
       "      <td>1924</td>\n",
       "      <td>1924</td>\n",
       "      <td>1924</td>\n",
       "      <td>1924</td>\n",
       "      <td>1924</td>\n",
       "      <td>1924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1924</td>\n",
       "      <td>1924</td>\n",
       "      <td>1924</td>\n",
       "      <td>1924</td>\n",
       "      <td>1924</td>\n",
       "      <td>1924</td>\n",
       "      <td>1924</td>\n",
       "      <td>1924</td>\n",
       "      <td>1924</td>\n",
       "      <td>1924</td>\n",
       "      <td>...</td>\n",
       "      <td>1924</td>\n",
       "      <td>1924</td>\n",
       "      <td>1924</td>\n",
       "      <td>1924</td>\n",
       "      <td>1924</td>\n",
       "      <td>1924</td>\n",
       "      <td>1924</td>\n",
       "      <td>1924</td>\n",
       "      <td>1924</td>\n",
       "      <td>1924</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 226 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       VAR002  VAR003  VAR004  VAR005  VAR006  VAR007  VAR008  VAR009  VAR010  \\\n",
       "label                                                                           \n",
       "0        1924    1924    1924    1924    1924    1924    1924    1924    1924   \n",
       "1        1924    1924    1924    1924    1924    1924    1924    1924    1924   \n",
       "\n",
       "       VAR011  ...  VAR219  VAR220  VAR221  VAR222  VAR223  VAR224  VAR225  \\\n",
       "label          ...                                                           \n",
       "0        1924  ...    1924    1924    1924    1924    1924    1924    1924   \n",
       "1        1924  ...    1924    1924    1924    1924    1924    1924    1924   \n",
       "\n",
       "       VAR226  VAR227  var178  \n",
       "label                          \n",
       "0        1924    1924    1924  \n",
       "1        1924    1924    1924  \n",
       "\n",
       "[2 rows x 226 columns]"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_samp, y_samp = RandomUnderSampler(random_state=0).fit_sample(df3, df1['label'])\n",
    "\n",
    "pd.pivot_table(X_samp, index = y_samp, aggfunc = 'count')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data분리\n",
    "\n",
    "#df2 #클래스 1:1 비율 안맞춘거\n",
    "#X_samp #클래스 1:1 비율 맞춘거 X_samp, index = y_samp\n",
    "#다중 클래스 df1['MRC_ID_DI']\n",
    "#이분류 클래스  df1['label']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "#x_train_all, x_test, y_train_all, y_test = train_test_split(df2, df1['MRC_ID_DI'], test_size=0.2, random_state=77,stratify=df1['MRC_ID_DI'])\n",
    "#클래스 비율 안맞춤\n",
    "#x_train_all, x_test, y_train_all, y_test = train_test_split(df2, df1['label'], test_size=0.2, random_state=77,stratify=df1['label'])\n",
    "#클래스 비율 맞춤\n",
    "x_train_all, x_test, y_train_all, y_test = train_test_split(X_samp, y_samp, test_size=0.2, random_state=77,stratify=y_samp)\n",
    "\n",
    "\n",
    "#validation dataset 분류  x , X_train_all\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train_all, y_train_all, test_size=0.22, random_state=77, stratify=y_train_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 442, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 442, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 442, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 442, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 442, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 442, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 442, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 10.0, 'penalty': 'l2', 'random_state': 77}\n",
      "0.7833333333333334\n",
      "로지스틱 회귀 훈련 정확도: 0.829\n",
      "로지스틱 회귀 검증 정확도: 0.816\n",
      "로지스틱 회귀 테스트 정확도: 0.791\n",
      "로지스틱 회귀 테스트 f1: 0.791\n",
      "Predicted    0    1\n",
      "Actual             \n",
      "0          291   94\n",
      "1           67  318\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.76      0.78       385\n",
      "           1       0.77      0.83      0.80       385\n",
      "\n",
      "    accuracy                           0.79       770\n",
      "   macro avg       0.79      0.79      0.79       770\n",
      "weighted avg       0.79      0.79      0.79       770\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "param_grid = {\n",
    "    \"C\":np.logspace(-3,3,7),\n",
    "    'penalty': ['l1','l2'],\n",
    "    'random_state' : [77] \n",
    "}\n",
    "\n",
    "clf = LogisticRegression(random_state=77)\n",
    "clf = GridSearchCV(clf, param_grid=param_grid) \n",
    "clf.fit(x_train, y_train)\n",
    "print(clf.best_params_)\n",
    "print(clf.best_score_)\n",
    "y_predt = clf.predict(x_train)\n",
    "print(\"로지스틱 회귀 훈련 정확도: {:.3f}\".format(accuracy_score(y_predt, y_train)))\n",
    "y_pred = clf.predict(x_val)\n",
    "print(\"로지스틱 회귀 검증 정확도: {:.3f}\".format(accuracy_score(y_pred, y_val)))\n",
    "y_pred = clf.predict(x_test)\n",
    "print(\"로지스틱 회귀 테스트 정확도: {:.3f}\".format(accuracy_score(y_pred, y_test)))\n",
    "print(\"로지스틱 회귀 테스트 f1: {:.3f}\".format(f1_score(y_pred, y_test, average='weighted')))\n",
    "\n",
    "confusion_matrix = pd.crosstab(y_test, y_pred, rownames=['Actual'], colnames=['Predicted'])\n",
    "print (confusion_matrix)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sn.heatmap(confusion_matrix, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.76      0.78       385\n",
      "           1       0.77      0.83      0.80       385\n",
      "\n",
      "    accuracy                           0.79       770\n",
      "   macro avg       0.79      0.79      0.79       770\n",
      "weighted avg       0.79      0.79      0.79       770\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.287261\n",
      "         Iterations: 35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/statsmodels/base/model.py:567: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warn(\"Maximum Likelihood optimization failed to converge. \"\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "lgt = sm.Logit(df1['label'],df2)\n",
    "result = lgt.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/statsmodels/base/model.py:1354: RuntimeWarning: invalid value encountered in sqrt\n",
      "  bse_ = np.sqrt(np.diag(self.cov_params()))\n",
      "/opt/anaconda3/lib/python3.8/site-packages/scipy/stats/_distn_infrastructure.py:1932: RuntimeWarning: invalid value encountered in less_equal\n",
      "  cond2 = cond0 & (x <= _a)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>label</td>      <th>  No. Observations:  </th>  <td> 10124</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>  <td>  9899</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>  <td>   224</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Fri, 11 Dec 2020</td> <th>  Pseudo R-squ.:     </th>  <td>0.4093</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>21:17:04</td>     <th>  Log-Likelihood:    </th> <td> -2908.2</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>             <td>False</td>      <th>  LL-Null:           </th> <td> -4923.2</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th>  <td> 0.000</td> \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "     <td></td>       <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR002</th> <td>    0.6300</td> <td>    0.712</td> <td>    0.885</td> <td> 0.376</td> <td>   -0.766</td> <td>    2.026</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR003</th> <td>   -1.3266</td> <td>    0.212</td> <td>   -6.262</td> <td> 0.000</td> <td>   -1.742</td> <td>   -0.911</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR004</th> <td>   -6.7817</td> <td>    3.534</td> <td>   -1.919</td> <td> 0.055</td> <td>  -13.708</td> <td>    0.144</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR005</th> <td>   -0.1424</td> <td>    0.624</td> <td>   -0.228</td> <td> 0.820</td> <td>   -1.366</td> <td>    1.081</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR006</th> <td>   -5.9594</td> <td>    3.851</td> <td>   -1.547</td> <td> 0.122</td> <td>  -13.508</td> <td>    1.589</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR007</th> <td>   -6.5307</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR008</th> <td>   -0.0775</td> <td>    0.121</td> <td>   -0.638</td> <td> 0.524</td> <td>   -0.316</td> <td>    0.161</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR009</th> <td>   11.7703</td> <td>    3.198</td> <td>    3.681</td> <td> 0.000</td> <td>    5.503</td> <td>   18.038</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR010</th> <td>    0.2516</td> <td>    0.178</td> <td>    1.415</td> <td> 0.157</td> <td>   -0.097</td> <td>    0.600</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR011</th> <td>   19.6019</td> <td>    6.397</td> <td>    3.064</td> <td> 0.002</td> <td>    7.063</td> <td>   32.140</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR012</th> <td>   -1.9076</td> <td>   10.762</td> <td>   -0.177</td> <td> 0.859</td> <td>  -23.001</td> <td>   19.186</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR013</th> <td>   -6.4866</td> <td>    7.472</td> <td>   -0.868</td> <td> 0.385</td> <td>  -21.131</td> <td>    8.158</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR014</th> <td>    9.0127</td> <td>   10.265</td> <td>    0.878</td> <td> 0.380</td> <td>  -11.105</td> <td>   29.131</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR015</th> <td>   -0.0578</td> <td>    0.090</td> <td>   -0.640</td> <td> 0.522</td> <td>   -0.235</td> <td>    0.119</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR016</th> <td>    0.3917</td> <td>    0.289</td> <td>    1.356</td> <td> 0.175</td> <td>   -0.175</td> <td>    0.958</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR017</th> <td>   -0.4235</td> <td>    0.301</td> <td>   -1.407</td> <td> 0.160</td> <td>   -1.014</td> <td>    0.167</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR018</th> <td>   -6.1334</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR019</th> <td>   -0.9144</td> <td>    1.966</td> <td>   -0.465</td> <td> 0.642</td> <td>   -4.768</td> <td>    2.939</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR020</th> <td>    0.1535</td> <td>    0.634</td> <td>    0.242</td> <td> 0.809</td> <td>   -1.089</td> <td>    1.396</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR021</th> <td>    2.2511</td> <td>    2.593</td> <td>    0.868</td> <td> 0.385</td> <td>   -2.831</td> <td>    7.333</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR022</th> <td>   -8.5347</td> <td>    5.051</td> <td>   -1.690</td> <td> 0.091</td> <td>  -18.434</td> <td>    1.365</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR023</th> <td>    0.7019</td> <td>    0.520</td> <td>    1.350</td> <td> 0.177</td> <td>   -0.317</td> <td>    1.721</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR024</th> <td>  -18.7840</td> <td>    8.280</td> <td>   -2.269</td> <td> 0.023</td> <td>  -35.012</td> <td>   -2.556</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR025</th> <td>    7.0991</td> <td>    2.363</td> <td>    3.005</td> <td> 0.003</td> <td>    2.468</td> <td>   11.730</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR026</th> <td>    0.2009</td> <td>    0.245</td> <td>    0.820</td> <td> 0.412</td> <td>   -0.279</td> <td>    0.681</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR027</th> <td>   16.6514</td> <td>    5.837</td> <td>    2.853</td> <td> 0.004</td> <td>    5.211</td> <td>   28.092</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR028</th> <td>   -0.4389</td> <td>    7.320</td> <td>   -0.060</td> <td> 0.952</td> <td>  -14.786</td> <td>   13.908</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR029</th> <td>   -5.3971</td> <td>    2.002</td> <td>   -2.696</td> <td> 0.007</td> <td>   -9.320</td> <td>   -1.474</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR030</th> <td>   -0.1241</td> <td>    0.580</td> <td>   -0.214</td> <td> 0.830</td> <td>   -1.261</td> <td>    1.012</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR031</th> <td>    0.3173</td> <td>    0.181</td> <td>    1.756</td> <td> 0.079</td> <td>   -0.037</td> <td>    0.671</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR032</th> <td>   22.0257</td> <td>   10.977</td> <td>    2.007</td> <td> 0.045</td> <td>    0.511</td> <td>   43.540</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR033</th> <td>    1.1173</td> <td>    0.646</td> <td>    1.728</td> <td> 0.084</td> <td>   -0.150</td> <td>    2.384</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR034</th> <td>   -7.3578</td> <td>    2.952</td> <td>   -2.493</td> <td> 0.013</td> <td>  -13.143</td> <td>   -1.572</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR035</th> <td>   -7.5018</td> <td>    3.956</td> <td>   -1.896</td> <td> 0.058</td> <td>  -15.256</td> <td>    0.252</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR036</th> <td>    0.2939</td> <td>    0.139</td> <td>    2.114</td> <td> 0.034</td> <td>    0.021</td> <td>    0.566</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR037</th> <td>   -3.4827</td> <td>    1.895</td> <td>   -1.837</td> <td> 0.066</td> <td>   -7.198</td> <td>    0.232</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR038</th> <td>   -3.5243</td> <td>    4.977</td> <td>   -0.708</td> <td> 0.479</td> <td>  -13.278</td> <td>    6.229</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR039</th> <td>  -11.6924</td> <td>   10.347</td> <td>   -1.130</td> <td> 0.258</td> <td>  -31.972</td> <td>    8.587</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR040</th> <td>    0.4298</td> <td>    1.353</td> <td>    0.318</td> <td> 0.751</td> <td>   -2.223</td> <td>    3.082</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR041</th> <td>   32.8924</td> <td>    7.813</td> <td>    4.210</td> <td> 0.000</td> <td>   17.580</td> <td>   48.205</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR042</th> <td>    0.6513</td> <td>    0.559</td> <td>    1.165</td> <td> 0.244</td> <td>   -0.445</td> <td>    1.747</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR043</th> <td>   -2.2240</td> <td>    1.988</td> <td>   -1.119</td> <td> 0.263</td> <td>   -6.120</td> <td>    1.672</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR044</th> <td>    0.2215</td> <td>    0.475</td> <td>    0.467</td> <td> 0.641</td> <td>   -0.709</td> <td>    1.152</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR045</th> <td>   -0.4719</td> <td>    0.660</td> <td>   -0.715</td> <td> 0.475</td> <td>   -1.766</td> <td>    0.823</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR046</th> <td>    9.8707</td> <td>    2.440</td> <td>    4.045</td> <td> 0.000</td> <td>    5.088</td> <td>   14.654</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR047</th> <td>    5.7132</td> <td>    2.994</td> <td>    1.908</td> <td> 0.056</td> <td>   -0.154</td> <td>   11.581</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR048</th> <td>   -2.3017</td> <td>    6.185</td> <td>   -0.372</td> <td> 0.710</td> <td>  -14.424</td> <td>    9.821</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR049</th> <td>    4.2566</td> <td>    3.507</td> <td>    1.214</td> <td> 0.225</td> <td>   -2.618</td> <td>   11.131</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR050</th> <td>    0.5458</td> <td>    2.503</td> <td>    0.218</td> <td> 0.827</td> <td>   -4.361</td> <td>    5.452</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR051</th> <td>   -0.0209</td> <td>    3.252</td> <td>   -0.006</td> <td> 0.995</td> <td>   -6.395</td> <td>    6.354</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR052</th> <td>    9.2099</td> <td>   11.850</td> <td>    0.777</td> <td> 0.437</td> <td>  -14.016</td> <td>   32.436</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR053</th> <td>   27.3262</td> <td>    6.702</td> <td>    4.077</td> <td> 0.000</td> <td>   14.191</td> <td>   40.462</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR054</th> <td>   -5.8622</td> <td>    6.626</td> <td>   -0.885</td> <td> 0.376</td> <td>  -18.848</td> <td>    7.124</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR055</th> <td>   -4.5470</td> <td>    7.550</td> <td>   -0.602</td> <td> 0.547</td> <td>  -19.345</td> <td>   10.251</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR056</th> <td>   -4.6693</td> <td>    2.221</td> <td>   -2.102</td> <td> 0.036</td> <td>   -9.023</td> <td>   -0.316</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR057</th> <td>   -6.6733</td> <td>    9.151</td> <td>   -0.729</td> <td> 0.466</td> <td>  -24.609</td> <td>   11.263</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR058</th> <td>  -12.6883</td> <td>    6.484</td> <td>   -1.957</td> <td> 0.050</td> <td>  -25.396</td> <td>    0.020</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR059</th> <td>   -6.0648</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR060</th> <td>   37.1705</td> <td>   16.310</td> <td>    2.279</td> <td> 0.023</td> <td>    5.203</td> <td>   69.138</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR061</th> <td>   11.3195</td> <td>    7.072</td> <td>    1.600</td> <td> 0.109</td> <td>   -2.542</td> <td>   25.181</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR062</th> <td>    0.0107</td> <td>    0.207</td> <td>    0.052</td> <td> 0.959</td> <td>   -0.395</td> <td>    0.416</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR063</th> <td>    5.5787</td> <td>    4.246</td> <td>    1.314</td> <td> 0.189</td> <td>   -2.743</td> <td>   13.901</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR064</th> <td>   -4.4950</td> <td>    1.838</td> <td>   -2.446</td> <td> 0.014</td> <td>   -8.097</td> <td>   -0.893</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR065</th> <td>    1.2265</td> <td>    0.576</td> <td>    2.130</td> <td> 0.033</td> <td>    0.098</td> <td>    2.355</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR066</th> <td>   -0.0160</td> <td>    0.087</td> <td>   -0.184</td> <td> 0.854</td> <td>   -0.186</td> <td>    0.154</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR067</th> <td>    0.2387</td> <td>    0.119</td> <td>    2.004</td> <td> 0.045</td> <td>    0.005</td> <td>    0.472</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR068</th> <td>   -2.1023</td> <td>    6.567</td> <td>   -0.320</td> <td> 0.749</td> <td>  -14.974</td> <td>   10.770</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR069</th> <td>   -4.9664</td> <td>    4.455</td> <td>   -1.115</td> <td> 0.265</td> <td>  -13.697</td> <td>    3.764</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR070</th> <td>   -0.2708</td> <td>    0.114</td> <td>   -2.377</td> <td> 0.017</td> <td>   -0.494</td> <td>   -0.048</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR071</th> <td>   10.1004</td> <td>  367.368</td> <td>    0.027</td> <td> 0.978</td> <td> -709.928</td> <td>  730.128</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR072</th> <td>   -0.9491</td> <td>    0.814</td> <td>   -1.166</td> <td> 0.244</td> <td>   -2.545</td> <td>    0.646</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR073</th> <td>   -7.9222</td> <td>    9.302</td> <td>   -0.852</td> <td> 0.394</td> <td>  -26.153</td> <td>   10.309</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR074</th> <td>   -1.6998</td> <td>    0.708</td> <td>   -2.401</td> <td> 0.016</td> <td>   -3.088</td> <td>   -0.312</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR075</th> <td>    0.7854</td> <td>    1.746</td> <td>    0.450</td> <td> 0.653</td> <td>   -2.637</td> <td>    4.208</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR076</th> <td>  -26.3072</td> <td>   15.252</td> <td>   -1.725</td> <td> 0.085</td> <td>  -56.200</td> <td>    3.586</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR077</th> <td>    9.8945</td> <td>  367.368</td> <td>    0.027</td> <td> 0.979</td> <td> -710.133</td> <td>  729.922</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR078</th> <td>   -0.3928</td> <td>    0.204</td> <td>   -1.930</td> <td> 0.054</td> <td>   -0.792</td> <td>    0.006</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR079</th> <td>   -9.8468</td> <td>    6.177</td> <td>   -1.594</td> <td> 0.111</td> <td>  -21.954</td> <td>    2.261</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR080</th> <td>   -0.4790</td> <td>    1.694</td> <td>   -0.283</td> <td> 0.777</td> <td>   -3.798</td> <td>    2.840</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR081</th> <td>   -1.8200</td> <td>    1.870</td> <td>   -0.973</td> <td> 0.330</td> <td>   -5.485</td> <td>    1.845</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR082</th> <td>    1.7862</td> <td>    1.116</td> <td>    1.601</td> <td> 0.109</td> <td>   -0.400</td> <td>    3.973</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR083</th> <td>   -1.1808</td> <td>    0.668</td> <td>   -1.767</td> <td> 0.077</td> <td>   -2.491</td> <td>    0.129</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR084</th> <td>    0.5046</td> <td>    0.317</td> <td>    1.590</td> <td> 0.112</td> <td>   -0.117</td> <td>    1.126</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR085</th> <td>   13.8635</td> <td>    6.751</td> <td>    2.053</td> <td> 0.040</td> <td>    0.631</td> <td>   27.096</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR086</th> <td>    0.7228</td> <td>    0.869</td> <td>    0.832</td> <td> 0.405</td> <td>   -0.980</td> <td>    2.426</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR087</th> <td>   -2.6692</td> <td>    8.957</td> <td>   -0.298</td> <td> 0.766</td> <td>  -20.226</td> <td>   14.887</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR088</th> <td>    0.2074</td> <td>    0.647</td> <td>    0.321</td> <td> 0.749</td> <td>   -1.061</td> <td>    1.476</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR089</th> <td>  -20.0827</td> <td>    8.248</td> <td>   -2.435</td> <td> 0.015</td> <td>  -36.249</td> <td>   -3.916</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR090</th> <td>    0.1335</td> <td>    0.134</td> <td>    0.995</td> <td> 0.320</td> <td>   -0.130</td> <td>    0.397</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR091</th> <td>  -14.8476</td> <td>    9.857</td> <td>   -1.506</td> <td> 0.132</td> <td>  -34.167</td> <td>    4.472</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR092</th> <td>  -12.7273</td> <td>    5.967</td> <td>   -2.133</td> <td> 0.033</td> <td>  -24.423</td> <td>   -1.032</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR093</th> <td>   -9.6512</td> <td>    3.225</td> <td>   -2.992</td> <td> 0.003</td> <td>  -15.973</td> <td>   -3.330</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR094</th> <td>   10.5979</td> <td>  367.368</td> <td>    0.029</td> <td> 0.977</td> <td> -709.430</td> <td>  730.626</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR095</th> <td>    9.4924</td> <td>   11.775</td> <td>    0.806</td> <td> 0.420</td> <td>  -13.587</td> <td>   32.572</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR096</th> <td>   10.0323</td> <td>  367.368</td> <td>    0.027</td> <td> 0.978</td> <td> -709.995</td> <td>  730.060</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR097</th> <td>   -6.2290</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR098</th> <td>   10.5036</td> <td>  367.368</td> <td>    0.029</td> <td> 0.977</td> <td> -709.524</td> <td>  730.532</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR099</th> <td>    0.5863</td> <td>    0.503</td> <td>    1.165</td> <td> 0.244</td> <td>   -0.400</td> <td>    1.573</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR100</th> <td>    0.4258</td> <td>    0.515</td> <td>    0.827</td> <td> 0.408</td> <td>   -0.583</td> <td>    1.435</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR101</th> <td>    0.9345</td> <td>    0.644</td> <td>    1.451</td> <td> 0.147</td> <td>   -0.328</td> <td>    2.197</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR102</th> <td>    1.2868</td> <td>    0.491</td> <td>    2.623</td> <td> 0.009</td> <td>    0.325</td> <td>    2.248</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR103</th> <td>   -0.5047</td> <td>    0.203</td> <td>   -2.491</td> <td> 0.013</td> <td>   -0.902</td> <td>   -0.108</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR104</th> <td>    5.7792</td> <td>    1.737</td> <td>    3.326</td> <td> 0.001</td> <td>    2.374</td> <td>    9.185</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR105</th> <td>    0.0665</td> <td>    0.430</td> <td>    0.155</td> <td> 0.877</td> <td>   -0.776</td> <td>    0.909</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR106</th> <td>   -4.4139</td> <td>    2.825</td> <td>   -1.563</td> <td> 0.118</td> <td>   -9.950</td> <td>    1.122</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR107</th> <td>   -5.4931</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR108</th> <td>    0.3718</td> <td>    0.482</td> <td>    0.772</td> <td> 0.440</td> <td>   -0.572</td> <td>    1.316</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR109</th> <td>   -1.6900</td> <td>    1.610</td> <td>   -1.050</td> <td> 0.294</td> <td>   -4.845</td> <td>    1.465</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR110</th> <td>   -0.7434</td> <td>    0.513</td> <td>   -1.449</td> <td> 0.147</td> <td>   -1.749</td> <td>    0.262</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR111</th> <td>   -5.8746</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR112</th> <td>    0.3683</td> <td>    0.492</td> <td>    0.749</td> <td> 0.454</td> <td>   -0.595</td> <td>    1.332</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR113</th> <td>    1.1769</td> <td>    0.763</td> <td>    1.543</td> <td> 0.123</td> <td>   -0.318</td> <td>    2.672</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR114</th> <td>  -46.1097</td> <td>   10.878</td> <td>   -4.239</td> <td> 0.000</td> <td>  -67.429</td> <td>  -24.790</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR115</th> <td>    5.9677</td> <td>    1.878</td> <td>    3.177</td> <td> 0.001</td> <td>    2.287</td> <td>    9.649</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR116</th> <td>   -3.9687</td> <td>    7.384</td> <td>   -0.537</td> <td> 0.591</td> <td>  -18.441</td> <td>   10.504</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR117</th> <td>   -0.2163</td> <td>    0.271</td> <td>   -0.799</td> <td> 0.424</td> <td>   -0.747</td> <td>    0.314</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR118</th> <td>   -3.1345</td> <td>    4.399</td> <td>   -0.712</td> <td> 0.476</td> <td>  -11.757</td> <td>    5.488</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR119</th> <td>  -30.5417</td> <td>   14.536</td> <td>   -2.101</td> <td> 0.036</td> <td>  -59.032</td> <td>   -2.051</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR120</th> <td>   -4.9984</td> <td>    6.700</td> <td>   -0.746</td> <td> 0.456</td> <td>  -18.130</td> <td>    8.133</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR121</th> <td>   12.4738</td> <td>    9.438</td> <td>    1.322</td> <td> 0.186</td> <td>   -6.025</td> <td>   30.972</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR122</th> <td>   -5.4452</td> <td>    3.330</td> <td>   -1.635</td> <td> 0.102</td> <td>  -11.971</td> <td>    1.081</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR123</th> <td>    4.7636</td> <td>   12.422</td> <td>    0.383</td> <td> 0.701</td> <td>  -19.582</td> <td>   29.110</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR124</th> <td>   -6.4348</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR125</th> <td>   -2.0826</td> <td>    4.752</td> <td>   -0.438</td> <td> 0.661</td> <td>  -11.395</td> <td>    7.230</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR126</th> <td>    2.1547</td> <td>    1.844</td> <td>    1.169</td> <td> 0.243</td> <td>   -1.459</td> <td>    5.768</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR127</th> <td>   -6.5302</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR128</th> <td>  -15.0087</td> <td>    5.772</td> <td>   -2.600</td> <td> 0.009</td> <td>  -26.321</td> <td>   -3.696</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR129</th> <td>    1.1487</td> <td>    0.455</td> <td>    2.526</td> <td> 0.012</td> <td>    0.257</td> <td>    2.040</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR130</th> <td>    1.5670</td> <td>    1.233</td> <td>    1.271</td> <td> 0.204</td> <td>   -0.849</td> <td>    3.983</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR131</th> <td>   -0.0975</td> <td>    0.572</td> <td>   -0.171</td> <td> 0.865</td> <td>   -1.218</td> <td>    1.023</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR132</th> <td>    0.7649</td> <td>    0.538</td> <td>    1.422</td> <td> 0.155</td> <td>   -0.289</td> <td>    1.819</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR133</th> <td>   -1.1150</td> <td>    2.254</td> <td>   -0.495</td> <td> 0.621</td> <td>   -5.533</td> <td>    3.302</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR134</th> <td>   -2.2764</td> <td>    5.159</td> <td>   -0.441</td> <td> 0.659</td> <td>  -12.388</td> <td>    7.836</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR135</th> <td>    1.3479</td> <td>    0.474</td> <td>    2.846</td> <td> 0.004</td> <td>    0.420</td> <td>    2.276</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR136</th> <td>    0.7398</td> <td>    0.521</td> <td>    1.419</td> <td> 0.156</td> <td>   -0.282</td> <td>    1.762</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR137</th> <td>   -5.7051</td> <td>    8.963</td> <td>   -0.637</td> <td> 0.524</td> <td>  -23.273</td> <td>   11.862</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR138</th> <td>    0.3397</td> <td>    0.131</td> <td>    2.589</td> <td> 0.010</td> <td>    0.082</td> <td>    0.597</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR139</th> <td>   13.8465</td> <td>    7.271</td> <td>    1.904</td> <td> 0.057</td> <td>   -0.404</td> <td>   28.097</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR140</th> <td>  -13.7135</td> <td>    6.903</td> <td>   -1.987</td> <td> 0.047</td> <td>  -27.243</td> <td>   -0.184</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR141</th> <td>    6.1056</td> <td>    1.983</td> <td>    3.079</td> <td> 0.002</td> <td>    2.219</td> <td>    9.992</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR142</th> <td>    0.1505</td> <td>    0.697</td> <td>    0.216</td> <td> 0.829</td> <td>   -1.215</td> <td>    1.516</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR143</th> <td>   10.6816</td> <td>  367.368</td> <td>    0.029</td> <td> 0.977</td> <td> -709.347</td> <td>  730.710</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR144</th> <td>    0.1798</td> <td>    0.170</td> <td>    1.058</td> <td> 0.290</td> <td>   -0.153</td> <td>    0.513</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR145</th> <td>   10.1769</td> <td>  367.368</td> <td>    0.028</td> <td> 0.978</td> <td> -709.851</td> <td>  730.205</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR146</th> <td>   -0.0667</td> <td>    0.480</td> <td>   -0.139</td> <td> 0.890</td> <td>   -1.008</td> <td>    0.875</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR147</th> <td>   -0.6875</td> <td>    4.164</td> <td>   -0.165</td> <td> 0.869</td> <td>   -8.848</td> <td>    7.473</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR148</th> <td>   10.0362</td> <td>  367.368</td> <td>    0.027</td> <td> 0.978</td> <td> -709.991</td> <td>  730.064</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR149</th> <td>    0.7307</td> <td>    1.119</td> <td>    0.653</td> <td> 0.514</td> <td>   -1.462</td> <td>    2.924</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR150</th> <td>   16.7208</td> <td>    8.326</td> <td>    2.008</td> <td> 0.045</td> <td>    0.403</td> <td>   33.039</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR151</th> <td>   -0.1983</td> <td>    0.493</td> <td>   -0.402</td> <td> 0.688</td> <td>   -1.165</td> <td>    0.768</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR152</th> <td>   -1.2804</td> <td>    9.289</td> <td>   -0.138</td> <td> 0.890</td> <td>  -19.487</td> <td>   16.926</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR153</th> <td>   -7.5375</td> <td>    2.963</td> <td>   -2.544</td> <td> 0.011</td> <td>  -13.345</td> <td>   -1.730</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR154</th> <td>    0.2919</td> <td>    5.850</td> <td>    0.050</td> <td> 0.960</td> <td>  -11.173</td> <td>   11.757</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR155</th> <td>   -0.9078</td> <td>    0.850</td> <td>   -1.068</td> <td> 0.286</td> <td>   -2.574</td> <td>    0.758</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR156</th> <td>    5.0209</td> <td>    2.512</td> <td>    1.999</td> <td> 0.046</td> <td>    0.097</td> <td>    9.945</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR157</th> <td>    5.9844</td> <td>    2.754</td> <td>    2.173</td> <td> 0.030</td> <td>    0.587</td> <td>   11.382</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR158</th> <td>   -6.8269</td> <td>    7.418</td> <td>   -0.920</td> <td> 0.357</td> <td>  -21.366</td> <td>    7.712</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR159</th> <td>    1.5144</td> <td>    4.314</td> <td>    0.351</td> <td> 0.726</td> <td>   -6.940</td> <td>    9.969</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR160</th> <td>   29.0959</td> <td>   11.493</td> <td>    2.532</td> <td> 0.011</td> <td>    6.569</td> <td>   51.622</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR161</th> <td>  -12.1830</td> <td>    7.652</td> <td>   -1.592</td> <td> 0.111</td> <td>  -27.180</td> <td>    2.814</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR162</th> <td>   -9.0125</td> <td>    8.547</td> <td>   -1.055</td> <td> 0.292</td> <td>  -25.764</td> <td>    7.739</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR163</th> <td>    1.9508</td> <td>    0.486</td> <td>    4.018</td> <td> 0.000</td> <td>    0.999</td> <td>    2.902</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR164</th> <td>  -14.1070</td> <td>    8.013</td> <td>   -1.761</td> <td> 0.078</td> <td>  -29.812</td> <td>    1.598</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR165</th> <td>    0.1981</td> <td>    0.291</td> <td>    0.681</td> <td> 0.496</td> <td>   -0.373</td> <td>    0.769</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR166</th> <td>    0.0416</td> <td>    1.172</td> <td>    0.035</td> <td> 0.972</td> <td>   -2.256</td> <td>    2.339</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR167</th> <td>   -0.1705</td> <td>    0.333</td> <td>   -0.511</td> <td> 0.609</td> <td>   -0.824</td> <td>    0.483</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR168</th> <td>  -33.0661</td> <td>   12.293</td> <td>   -2.690</td> <td> 0.007</td> <td>  -57.160</td> <td>   -8.973</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR169</th> <td>    0.3315</td> <td>    1.923</td> <td>    0.172</td> <td> 0.863</td> <td>   -3.438</td> <td>    4.101</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR170</th> <td>   -2.8360</td> <td>    1.498</td> <td>   -1.893</td> <td> 0.058</td> <td>   -5.772</td> <td>    0.100</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR171</th> <td>    0.0817</td> <td>    0.117</td> <td>    0.696</td> <td> 0.487</td> <td>   -0.149</td> <td>    0.312</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR172</th> <td>   -0.0433</td> <td>    2.903</td> <td>   -0.015</td> <td> 0.988</td> <td>   -5.733</td> <td>    5.647</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR173</th> <td>   -2.4072</td> <td>    0.810</td> <td>   -2.972</td> <td> 0.003</td> <td>   -3.994</td> <td>   -0.820</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR174</th> <td>   -3.5297</td> <td>    2.400</td> <td>   -1.470</td> <td> 0.141</td> <td>   -8.234</td> <td>    1.175</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR175</th> <td>   -0.3743</td> <td>    1.347</td> <td>   -0.278</td> <td> 0.781</td> <td>   -3.015</td> <td>    2.266</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR176</th> <td>   -0.1368</td> <td>   12.657</td> <td>   -0.011</td> <td> 0.991</td> <td>  -24.944</td> <td>   24.670</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR177</th> <td>    9.6083</td> <td>  367.368</td> <td>    0.026</td> <td> 0.979</td> <td> -710.419</td> <td>  729.636</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>var178</th> <td>   -0.1292</td> <td>    0.148</td> <td>   -0.876</td> <td> 0.381</td> <td>   -0.418</td> <td>    0.160</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR179</th> <td>   -6.0546</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR180</th> <td>    0.7118</td> <td>    0.503</td> <td>    1.414</td> <td> 0.157</td> <td>   -0.275</td> <td>    1.698</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR181</th> <td>    0.1917</td> <td>    0.434</td> <td>    0.441</td> <td> 0.659</td> <td>   -0.660</td> <td>    1.043</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR182</th> <td>   -1.7817</td> <td>    1.036</td> <td>   -1.719</td> <td> 0.086</td> <td>   -3.813</td> <td>    0.249</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR183</th> <td>    0.0296</td> <td>    0.944</td> <td>    0.031</td> <td> 0.975</td> <td>   -1.820</td> <td>    1.880</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR184</th> <td>   -4.3313</td> <td>    3.355</td> <td>   -1.291</td> <td> 0.197</td> <td>  -10.907</td> <td>    2.244</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR185</th> <td>   -7.5031</td> <td>    5.060</td> <td>   -1.483</td> <td> 0.138</td> <td>  -17.420</td> <td>    2.414</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR186</th> <td>  -14.8329</td> <td>   10.552</td> <td>   -1.406</td> <td> 0.160</td> <td>  -35.514</td> <td>    5.849</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR187</th> <td>    3.3268</td> <td>    1.715</td> <td>    1.940</td> <td> 0.052</td> <td>   -0.034</td> <td>    6.688</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR188</th> <td>    3.3877</td> <td>    1.213</td> <td>    2.794</td> <td> 0.005</td> <td>    1.011</td> <td>    5.764</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR189</th> <td>  -12.1970</td> <td>    7.209</td> <td>   -1.692</td> <td> 0.091</td> <td>  -26.327</td> <td>    1.933</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR190</th> <td>    0.3956</td> <td>    4.124</td> <td>    0.096</td> <td> 0.924</td> <td>   -7.686</td> <td>    8.478</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR191</th> <td>    6.0621</td> <td>    5.689</td> <td>    1.066</td> <td> 0.287</td> <td>   -5.089</td> <td>   17.213</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR192</th> <td>   -0.4251</td> <td>    0.180</td> <td>   -2.359</td> <td> 0.018</td> <td>   -0.778</td> <td>   -0.072</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR193</th> <td>    0.2984</td> <td>    0.493</td> <td>    0.606</td> <td> 0.545</td> <td>   -0.667</td> <td>    1.264</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR194</th> <td>    1.0256</td> <td>    0.482</td> <td>    2.130</td> <td> 0.033</td> <td>    0.082</td> <td>    1.969</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR195</th> <td>    0.5219</td> <td>    0.467</td> <td>    1.118</td> <td> 0.263</td> <td>   -0.393</td> <td>    1.437</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR196</th> <td>   -0.5348</td> <td>    6.677</td> <td>   -0.080</td> <td> 0.936</td> <td>  -13.621</td> <td>   12.552</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR197</th> <td>   -0.7165</td> <td>    0.536</td> <td>   -1.336</td> <td> 0.181</td> <td>   -1.767</td> <td>    0.334</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR198</th> <td>  -13.1226</td> <td>    6.590</td> <td>   -1.991</td> <td> 0.046</td> <td>  -26.038</td> <td>   -0.207</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR199</th> <td>   -5.9710</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR200</th> <td>   -6.4443</td> <td>    2.169</td> <td>   -2.971</td> <td> 0.003</td> <td>  -10.695</td> <td>   -2.193</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR201</th> <td>    0.1541</td> <td>    0.657</td> <td>    0.234</td> <td> 0.815</td> <td>   -1.134</td> <td>    1.442</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR202</th> <td>   -2.1978</td> <td>    7.215</td> <td>   -0.305</td> <td> 0.761</td> <td>  -16.339</td> <td>   11.943</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR203</th> <td>  -12.6240</td> <td>   11.115</td> <td>   -1.136</td> <td> 0.256</td> <td>  -34.409</td> <td>    9.161</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR204</th> <td>  -18.4998</td> <td>    9.972</td> <td>   -1.855</td> <td> 0.064</td> <td>  -38.045</td> <td>    1.045</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR205</th> <td>   -4.5359</td> <td>    1.297</td> <td>   -3.498</td> <td> 0.000</td> <td>   -7.077</td> <td>   -1.995</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR206</th> <td>   -0.0964</td> <td>    0.485</td> <td>   -0.199</td> <td> 0.842</td> <td>   -1.046</td> <td>    0.854</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR207</th> <td>    1.1311</td> <td>    0.529</td> <td>    2.139</td> <td> 0.032</td> <td>    0.094</td> <td>    2.168</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR208</th> <td>   10.0062</td> <td>  367.368</td> <td>    0.027</td> <td> 0.978</td> <td> -710.021</td> <td>  730.034</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR209</th> <td>   -0.9075</td> <td>    1.660</td> <td>   -0.547</td> <td> 0.585</td> <td>   -4.161</td> <td>    2.346</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR210</th> <td>    2.7222</td> <td>    1.449</td> <td>    1.879</td> <td> 0.060</td> <td>   -0.117</td> <td>    5.562</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR211</th> <td>    2.7477</td> <td>    2.053</td> <td>    1.339</td> <td> 0.181</td> <td>   -1.275</td> <td>    6.771</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR212</th> <td>    1.0285</td> <td>    0.669</td> <td>    1.537</td> <td> 0.124</td> <td>   -0.283</td> <td>    2.340</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR213</th> <td>    0.1291</td> <td>    0.521</td> <td>    0.248</td> <td> 0.805</td> <td>   -0.893</td> <td>    1.151</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR214</th> <td>   -4.0235</td> <td>    2.490</td> <td>   -1.616</td> <td> 0.106</td> <td>   -8.904</td> <td>    0.857</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR215</th> <td>    1.8856</td> <td>    2.392</td> <td>    0.788</td> <td> 0.430</td> <td>   -2.802</td> <td>    6.573</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR216</th> <td>  -29.7219</td> <td>   19.474</td> <td>   -1.526</td> <td> 0.127</td> <td>  -67.890</td> <td>    8.446</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR217</th> <td>   16.4710</td> <td>   10.655</td> <td>    1.546</td> <td> 0.122</td> <td>   -4.411</td> <td>   37.353</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR218</th> <td>    1.6599</td> <td>    3.441</td> <td>    0.482</td> <td> 0.630</td> <td>   -5.085</td> <td>    8.405</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR219</th> <td>   -3.4808</td> <td>    2.282</td> <td>   -1.526</td> <td> 0.127</td> <td>   -7.953</td> <td>    0.991</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR220</th> <td>    0.9565</td> <td>    0.518</td> <td>    1.846</td> <td> 0.065</td> <td>   -0.059</td> <td>    1.972</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR221</th> <td>    0.4543</td> <td>    0.578</td> <td>    0.786</td> <td> 0.432</td> <td>   -0.678</td> <td>    1.587</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR222</th> <td>   -7.2879</td> <td>    3.080</td> <td>   -2.366</td> <td> 0.018</td> <td>  -13.324</td> <td>   -1.252</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR223</th> <td>    0.4914</td> <td>    0.565</td> <td>    0.870</td> <td> 0.385</td> <td>   -0.616</td> <td>    1.599</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR224</th> <td>   -0.1486</td> <td>    0.726</td> <td>   -0.205</td> <td> 0.838</td> <td>   -1.571</td> <td>    1.273</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR225</th> <td>    0.5537</td> <td>    0.464</td> <td>    1.194</td> <td> 0.233</td> <td>   -0.355</td> <td>    1.463</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR226</th> <td>    4.1995</td> <td>    1.610</td> <td>    2.608</td> <td> 0.009</td> <td>    1.044</td> <td>    7.355</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VAR227</th> <td>    8.3271</td> <td>    5.070</td> <td>    1.643</td> <td> 0.100</td> <td>   -1.609</td> <td>   18.264</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:                  label   No. Observations:                10124\n",
       "Model:                          Logit   Df Residuals:                     9899\n",
       "Method:                           MLE   Df Model:                          224\n",
       "Date:                Fri, 11 Dec 2020   Pseudo R-squ.:                  0.4093\n",
       "Time:                        21:17:04   Log-Likelihood:                -2908.2\n",
       "converged:                      False   LL-Null:                       -4923.2\n",
       "Covariance Type:            nonrobust   LLR p-value:                     0.000\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "VAR002         0.6300      0.712      0.885      0.376      -0.766       2.026\n",
       "VAR003        -1.3266      0.212     -6.262      0.000      -1.742      -0.911\n",
       "VAR004        -6.7817      3.534     -1.919      0.055     -13.708       0.144\n",
       "VAR005        -0.1424      0.624     -0.228      0.820      -1.366       1.081\n",
       "VAR006        -5.9594      3.851     -1.547      0.122     -13.508       1.589\n",
       "VAR007        -6.5307        nan        nan        nan         nan         nan\n",
       "VAR008        -0.0775      0.121     -0.638      0.524      -0.316       0.161\n",
       "VAR009        11.7703      3.198      3.681      0.000       5.503      18.038\n",
       "VAR010         0.2516      0.178      1.415      0.157      -0.097       0.600\n",
       "VAR011        19.6019      6.397      3.064      0.002       7.063      32.140\n",
       "VAR012        -1.9076     10.762     -0.177      0.859     -23.001      19.186\n",
       "VAR013        -6.4866      7.472     -0.868      0.385     -21.131       8.158\n",
       "VAR014         9.0127     10.265      0.878      0.380     -11.105      29.131\n",
       "VAR015        -0.0578      0.090     -0.640      0.522      -0.235       0.119\n",
       "VAR016         0.3917      0.289      1.356      0.175      -0.175       0.958\n",
       "VAR017        -0.4235      0.301     -1.407      0.160      -1.014       0.167\n",
       "VAR018        -6.1334        nan        nan        nan         nan         nan\n",
       "VAR019        -0.9144      1.966     -0.465      0.642      -4.768       2.939\n",
       "VAR020         0.1535      0.634      0.242      0.809      -1.089       1.396\n",
       "VAR021         2.2511      2.593      0.868      0.385      -2.831       7.333\n",
       "VAR022        -8.5347      5.051     -1.690      0.091     -18.434       1.365\n",
       "VAR023         0.7019      0.520      1.350      0.177      -0.317       1.721\n",
       "VAR024       -18.7840      8.280     -2.269      0.023     -35.012      -2.556\n",
       "VAR025         7.0991      2.363      3.005      0.003       2.468      11.730\n",
       "VAR026         0.2009      0.245      0.820      0.412      -0.279       0.681\n",
       "VAR027        16.6514      5.837      2.853      0.004       5.211      28.092\n",
       "VAR028        -0.4389      7.320     -0.060      0.952     -14.786      13.908\n",
       "VAR029        -5.3971      2.002     -2.696      0.007      -9.320      -1.474\n",
       "VAR030        -0.1241      0.580     -0.214      0.830      -1.261       1.012\n",
       "VAR031         0.3173      0.181      1.756      0.079      -0.037       0.671\n",
       "VAR032        22.0257     10.977      2.007      0.045       0.511      43.540\n",
       "VAR033         1.1173      0.646      1.728      0.084      -0.150       2.384\n",
       "VAR034        -7.3578      2.952     -2.493      0.013     -13.143      -1.572\n",
       "VAR035        -7.5018      3.956     -1.896      0.058     -15.256       0.252\n",
       "VAR036         0.2939      0.139      2.114      0.034       0.021       0.566\n",
       "VAR037        -3.4827      1.895     -1.837      0.066      -7.198       0.232\n",
       "VAR038        -3.5243      4.977     -0.708      0.479     -13.278       6.229\n",
       "VAR039       -11.6924     10.347     -1.130      0.258     -31.972       8.587\n",
       "VAR040         0.4298      1.353      0.318      0.751      -2.223       3.082\n",
       "VAR041        32.8924      7.813      4.210      0.000      17.580      48.205\n",
       "VAR042         0.6513      0.559      1.165      0.244      -0.445       1.747\n",
       "VAR043        -2.2240      1.988     -1.119      0.263      -6.120       1.672\n",
       "VAR044         0.2215      0.475      0.467      0.641      -0.709       1.152\n",
       "VAR045        -0.4719      0.660     -0.715      0.475      -1.766       0.823\n",
       "VAR046         9.8707      2.440      4.045      0.000       5.088      14.654\n",
       "VAR047         5.7132      2.994      1.908      0.056      -0.154      11.581\n",
       "VAR048        -2.3017      6.185     -0.372      0.710     -14.424       9.821\n",
       "VAR049         4.2566      3.507      1.214      0.225      -2.618      11.131\n",
       "VAR050         0.5458      2.503      0.218      0.827      -4.361       5.452\n",
       "VAR051        -0.0209      3.252     -0.006      0.995      -6.395       6.354\n",
       "VAR052         9.2099     11.850      0.777      0.437     -14.016      32.436\n",
       "VAR053        27.3262      6.702      4.077      0.000      14.191      40.462\n",
       "VAR054        -5.8622      6.626     -0.885      0.376     -18.848       7.124\n",
       "VAR055        -4.5470      7.550     -0.602      0.547     -19.345      10.251\n",
       "VAR056        -4.6693      2.221     -2.102      0.036      -9.023      -0.316\n",
       "VAR057        -6.6733      9.151     -0.729      0.466     -24.609      11.263\n",
       "VAR058       -12.6883      6.484     -1.957      0.050     -25.396       0.020\n",
       "VAR059        -6.0648        nan        nan        nan         nan         nan\n",
       "VAR060        37.1705     16.310      2.279      0.023       5.203      69.138\n",
       "VAR061        11.3195      7.072      1.600      0.109      -2.542      25.181\n",
       "VAR062         0.0107      0.207      0.052      0.959      -0.395       0.416\n",
       "VAR063         5.5787      4.246      1.314      0.189      -2.743      13.901\n",
       "VAR064        -4.4950      1.838     -2.446      0.014      -8.097      -0.893\n",
       "VAR065         1.2265      0.576      2.130      0.033       0.098       2.355\n",
       "VAR066        -0.0160      0.087     -0.184      0.854      -0.186       0.154\n",
       "VAR067         0.2387      0.119      2.004      0.045       0.005       0.472\n",
       "VAR068        -2.1023      6.567     -0.320      0.749     -14.974      10.770\n",
       "VAR069        -4.9664      4.455     -1.115      0.265     -13.697       3.764\n",
       "VAR070        -0.2708      0.114     -2.377      0.017      -0.494      -0.048\n",
       "VAR071        10.1004    367.368      0.027      0.978    -709.928     730.128\n",
       "VAR072        -0.9491      0.814     -1.166      0.244      -2.545       0.646\n",
       "VAR073        -7.9222      9.302     -0.852      0.394     -26.153      10.309\n",
       "VAR074        -1.6998      0.708     -2.401      0.016      -3.088      -0.312\n",
       "VAR075         0.7854      1.746      0.450      0.653      -2.637       4.208\n",
       "VAR076       -26.3072     15.252     -1.725      0.085     -56.200       3.586\n",
       "VAR077         9.8945    367.368      0.027      0.979    -710.133     729.922\n",
       "VAR078        -0.3928      0.204     -1.930      0.054      -0.792       0.006\n",
       "VAR079        -9.8468      6.177     -1.594      0.111     -21.954       2.261\n",
       "VAR080        -0.4790      1.694     -0.283      0.777      -3.798       2.840\n",
       "VAR081        -1.8200      1.870     -0.973      0.330      -5.485       1.845\n",
       "VAR082         1.7862      1.116      1.601      0.109      -0.400       3.973\n",
       "VAR083        -1.1808      0.668     -1.767      0.077      -2.491       0.129\n",
       "VAR084         0.5046      0.317      1.590      0.112      -0.117       1.126\n",
       "VAR085        13.8635      6.751      2.053      0.040       0.631      27.096\n",
       "VAR086         0.7228      0.869      0.832      0.405      -0.980       2.426\n",
       "VAR087        -2.6692      8.957     -0.298      0.766     -20.226      14.887\n",
       "VAR088         0.2074      0.647      0.321      0.749      -1.061       1.476\n",
       "VAR089       -20.0827      8.248     -2.435      0.015     -36.249      -3.916\n",
       "VAR090         0.1335      0.134      0.995      0.320      -0.130       0.397\n",
       "VAR091       -14.8476      9.857     -1.506      0.132     -34.167       4.472\n",
       "VAR092       -12.7273      5.967     -2.133      0.033     -24.423      -1.032\n",
       "VAR093        -9.6512      3.225     -2.992      0.003     -15.973      -3.330\n",
       "VAR094        10.5979    367.368      0.029      0.977    -709.430     730.626\n",
       "VAR095         9.4924     11.775      0.806      0.420     -13.587      32.572\n",
       "VAR096        10.0323    367.368      0.027      0.978    -709.995     730.060\n",
       "VAR097        -6.2290        nan        nan        nan         nan         nan\n",
       "VAR098        10.5036    367.368      0.029      0.977    -709.524     730.532\n",
       "VAR099         0.5863      0.503      1.165      0.244      -0.400       1.573\n",
       "VAR100         0.4258      0.515      0.827      0.408      -0.583       1.435\n",
       "VAR101         0.9345      0.644      1.451      0.147      -0.328       2.197\n",
       "VAR102         1.2868      0.491      2.623      0.009       0.325       2.248\n",
       "VAR103        -0.5047      0.203     -2.491      0.013      -0.902      -0.108\n",
       "VAR104         5.7792      1.737      3.326      0.001       2.374       9.185\n",
       "VAR105         0.0665      0.430      0.155      0.877      -0.776       0.909\n",
       "VAR106        -4.4139      2.825     -1.563      0.118      -9.950       1.122\n",
       "VAR107        -5.4931        nan        nan        nan         nan         nan\n",
       "VAR108         0.3718      0.482      0.772      0.440      -0.572       1.316\n",
       "VAR109        -1.6900      1.610     -1.050      0.294      -4.845       1.465\n",
       "VAR110        -0.7434      0.513     -1.449      0.147      -1.749       0.262\n",
       "VAR111        -5.8746        nan        nan        nan         nan         nan\n",
       "VAR112         0.3683      0.492      0.749      0.454      -0.595       1.332\n",
       "VAR113         1.1769      0.763      1.543      0.123      -0.318       2.672\n",
       "VAR114       -46.1097     10.878     -4.239      0.000     -67.429     -24.790\n",
       "VAR115         5.9677      1.878      3.177      0.001       2.287       9.649\n",
       "VAR116        -3.9687      7.384     -0.537      0.591     -18.441      10.504\n",
       "VAR117        -0.2163      0.271     -0.799      0.424      -0.747       0.314\n",
       "VAR118        -3.1345      4.399     -0.712      0.476     -11.757       5.488\n",
       "VAR119       -30.5417     14.536     -2.101      0.036     -59.032      -2.051\n",
       "VAR120        -4.9984      6.700     -0.746      0.456     -18.130       8.133\n",
       "VAR121        12.4738      9.438      1.322      0.186      -6.025      30.972\n",
       "VAR122        -5.4452      3.330     -1.635      0.102     -11.971       1.081\n",
       "VAR123         4.7636     12.422      0.383      0.701     -19.582      29.110\n",
       "VAR124        -6.4348        nan        nan        nan         nan         nan\n",
       "VAR125        -2.0826      4.752     -0.438      0.661     -11.395       7.230\n",
       "VAR126         2.1547      1.844      1.169      0.243      -1.459       5.768\n",
       "VAR127        -6.5302        nan        nan        nan         nan         nan\n",
       "VAR128       -15.0087      5.772     -2.600      0.009     -26.321      -3.696\n",
       "VAR129         1.1487      0.455      2.526      0.012       0.257       2.040\n",
       "VAR130         1.5670      1.233      1.271      0.204      -0.849       3.983\n",
       "VAR131        -0.0975      0.572     -0.171      0.865      -1.218       1.023\n",
       "VAR132         0.7649      0.538      1.422      0.155      -0.289       1.819\n",
       "VAR133        -1.1150      2.254     -0.495      0.621      -5.533       3.302\n",
       "VAR134        -2.2764      5.159     -0.441      0.659     -12.388       7.836\n",
       "VAR135         1.3479      0.474      2.846      0.004       0.420       2.276\n",
       "VAR136         0.7398      0.521      1.419      0.156      -0.282       1.762\n",
       "VAR137        -5.7051      8.963     -0.637      0.524     -23.273      11.862\n",
       "VAR138         0.3397      0.131      2.589      0.010       0.082       0.597\n",
       "VAR139        13.8465      7.271      1.904      0.057      -0.404      28.097\n",
       "VAR140       -13.7135      6.903     -1.987      0.047     -27.243      -0.184\n",
       "VAR141         6.1056      1.983      3.079      0.002       2.219       9.992\n",
       "VAR142         0.1505      0.697      0.216      0.829      -1.215       1.516\n",
       "VAR143        10.6816    367.368      0.029      0.977    -709.347     730.710\n",
       "VAR144         0.1798      0.170      1.058      0.290      -0.153       0.513\n",
       "VAR145        10.1769    367.368      0.028      0.978    -709.851     730.205\n",
       "VAR146        -0.0667      0.480     -0.139      0.890      -1.008       0.875\n",
       "VAR147        -0.6875      4.164     -0.165      0.869      -8.848       7.473\n",
       "VAR148        10.0362    367.368      0.027      0.978    -709.991     730.064\n",
       "VAR149         0.7307      1.119      0.653      0.514      -1.462       2.924\n",
       "VAR150        16.7208      8.326      2.008      0.045       0.403      33.039\n",
       "VAR151        -0.1983      0.493     -0.402      0.688      -1.165       0.768\n",
       "VAR152        -1.2804      9.289     -0.138      0.890     -19.487      16.926\n",
       "VAR153        -7.5375      2.963     -2.544      0.011     -13.345      -1.730\n",
       "VAR154         0.2919      5.850      0.050      0.960     -11.173      11.757\n",
       "VAR155        -0.9078      0.850     -1.068      0.286      -2.574       0.758\n",
       "VAR156         5.0209      2.512      1.999      0.046       0.097       9.945\n",
       "VAR157         5.9844      2.754      2.173      0.030       0.587      11.382\n",
       "VAR158        -6.8269      7.418     -0.920      0.357     -21.366       7.712\n",
       "VAR159         1.5144      4.314      0.351      0.726      -6.940       9.969\n",
       "VAR160        29.0959     11.493      2.532      0.011       6.569      51.622\n",
       "VAR161       -12.1830      7.652     -1.592      0.111     -27.180       2.814\n",
       "VAR162        -9.0125      8.547     -1.055      0.292     -25.764       7.739\n",
       "VAR163         1.9508      0.486      4.018      0.000       0.999       2.902\n",
       "VAR164       -14.1070      8.013     -1.761      0.078     -29.812       1.598\n",
       "VAR165         0.1981      0.291      0.681      0.496      -0.373       0.769\n",
       "VAR166         0.0416      1.172      0.035      0.972      -2.256       2.339\n",
       "VAR167        -0.1705      0.333     -0.511      0.609      -0.824       0.483\n",
       "VAR168       -33.0661     12.293     -2.690      0.007     -57.160      -8.973\n",
       "VAR169         0.3315      1.923      0.172      0.863      -3.438       4.101\n",
       "VAR170        -2.8360      1.498     -1.893      0.058      -5.772       0.100\n",
       "VAR171         0.0817      0.117      0.696      0.487      -0.149       0.312\n",
       "VAR172        -0.0433      2.903     -0.015      0.988      -5.733       5.647\n",
       "VAR173        -2.4072      0.810     -2.972      0.003      -3.994      -0.820\n",
       "VAR174        -3.5297      2.400     -1.470      0.141      -8.234       1.175\n",
       "VAR175        -0.3743      1.347     -0.278      0.781      -3.015       2.266\n",
       "VAR176        -0.1368     12.657     -0.011      0.991     -24.944      24.670\n",
       "VAR177         9.6083    367.368      0.026      0.979    -710.419     729.636\n",
       "var178        -0.1292      0.148     -0.876      0.381      -0.418       0.160\n",
       "VAR179        -6.0546        nan        nan        nan         nan         nan\n",
       "VAR180         0.7118      0.503      1.414      0.157      -0.275       1.698\n",
       "VAR181         0.1917      0.434      0.441      0.659      -0.660       1.043\n",
       "VAR182        -1.7817      1.036     -1.719      0.086      -3.813       0.249\n",
       "VAR183         0.0296      0.944      0.031      0.975      -1.820       1.880\n",
       "VAR184        -4.3313      3.355     -1.291      0.197     -10.907       2.244\n",
       "VAR185        -7.5031      5.060     -1.483      0.138     -17.420       2.414\n",
       "VAR186       -14.8329     10.552     -1.406      0.160     -35.514       5.849\n",
       "VAR187         3.3268      1.715      1.940      0.052      -0.034       6.688\n",
       "VAR188         3.3877      1.213      2.794      0.005       1.011       5.764\n",
       "VAR189       -12.1970      7.209     -1.692      0.091     -26.327       1.933\n",
       "VAR190         0.3956      4.124      0.096      0.924      -7.686       8.478\n",
       "VAR191         6.0621      5.689      1.066      0.287      -5.089      17.213\n",
       "VAR192        -0.4251      0.180     -2.359      0.018      -0.778      -0.072\n",
       "VAR193         0.2984      0.493      0.606      0.545      -0.667       1.264\n",
       "VAR194         1.0256      0.482      2.130      0.033       0.082       1.969\n",
       "VAR195         0.5219      0.467      1.118      0.263      -0.393       1.437\n",
       "VAR196        -0.5348      6.677     -0.080      0.936     -13.621      12.552\n",
       "VAR197        -0.7165      0.536     -1.336      0.181      -1.767       0.334\n",
       "VAR198       -13.1226      6.590     -1.991      0.046     -26.038      -0.207\n",
       "VAR199        -5.9710        nan        nan        nan         nan         nan\n",
       "VAR200        -6.4443      2.169     -2.971      0.003     -10.695      -2.193\n",
       "VAR201         0.1541      0.657      0.234      0.815      -1.134       1.442\n",
       "VAR202        -2.1978      7.215     -0.305      0.761     -16.339      11.943\n",
       "VAR203       -12.6240     11.115     -1.136      0.256     -34.409       9.161\n",
       "VAR204       -18.4998      9.972     -1.855      0.064     -38.045       1.045\n",
       "VAR205        -4.5359      1.297     -3.498      0.000      -7.077      -1.995\n",
       "VAR206        -0.0964      0.485     -0.199      0.842      -1.046       0.854\n",
       "VAR207         1.1311      0.529      2.139      0.032       0.094       2.168\n",
       "VAR208        10.0062    367.368      0.027      0.978    -710.021     730.034\n",
       "VAR209        -0.9075      1.660     -0.547      0.585      -4.161       2.346\n",
       "VAR210         2.7222      1.449      1.879      0.060      -0.117       5.562\n",
       "VAR211         2.7477      2.053      1.339      0.181      -1.275       6.771\n",
       "VAR212         1.0285      0.669      1.537      0.124      -0.283       2.340\n",
       "VAR213         0.1291      0.521      0.248      0.805      -0.893       1.151\n",
       "VAR214        -4.0235      2.490     -1.616      0.106      -8.904       0.857\n",
       "VAR215         1.8856      2.392      0.788      0.430      -2.802       6.573\n",
       "VAR216       -29.7219     19.474     -1.526      0.127     -67.890       8.446\n",
       "VAR217        16.4710     10.655      1.546      0.122      -4.411      37.353\n",
       "VAR218         1.6599      3.441      0.482      0.630      -5.085       8.405\n",
       "VAR219        -3.4808      2.282     -1.526      0.127      -7.953       0.991\n",
       "VAR220         0.9565      0.518      1.846      0.065      -0.059       1.972\n",
       "VAR221         0.4543      0.578      0.786      0.432      -0.678       1.587\n",
       "VAR222        -7.2879      3.080     -2.366      0.018     -13.324      -1.252\n",
       "VAR223         0.4914      0.565      0.870      0.385      -0.616       1.599\n",
       "VAR224        -0.1486      0.726     -0.205      0.838      -1.571       1.273\n",
       "VAR225         0.5537      0.464      1.194      0.233      -0.355       1.463\n",
       "VAR226         4.1995      1.610      2.608      0.009       1.044       7.355\n",
       "VAR227         8.3271      5.070      1.643      0.100      -1.609      18.264\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(result.pvalues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['VAR051', 'VAR176', 'VAR172', 'VAR177', 'VAR077', 'VAR208', 'VAR096',\n",
       "       'VAR148', 'VAR071', 'VAR145', 'VAR098', 'VAR094', 'VAR143', 'VAR183',\n",
       "       'VAR166', 'VAR154', 'VAR062', 'VAR028', 'VAR196', 'VAR190', 'VAR152',\n",
       "       'VAR146', 'VAR105', 'VAR147', 'VAR131', 'VAR169', 'VAR012', 'VAR066',\n",
       "       'VAR206', 'VAR224', 'VAR030', 'VAR142', 'VAR050', 'VAR005', 'VAR201',\n",
       "       'VAR020', 'VAR213', 'VAR175', 'VAR080', 'VAR087', 'VAR202', 'VAR040',\n",
       "       'VAR068', 'VAR088', 'VAR159', 'VAR048', 'VAR123', 'VAR151', 'VAR125',\n",
       "       'VAR181', 'VAR134', 'VAR075', 'VAR019', 'VAR044', 'VAR218', 'VAR133',\n",
       "       'VAR167', 'VAR116', 'VAR209', 'VAR055', 'VAR193', 'VAR137', 'VAR008',\n",
       "       'VAR015', 'VAR149', 'VAR165', 'VAR171', 'VAR038', 'VAR118', 'VAR045',\n",
       "       'VAR057', 'VAR120', 'VAR112', 'VAR108', 'VAR052', 'VAR221', 'VAR215',\n",
       "       'VAR117', 'VAR095', 'VAR026', 'VAR100', 'VAR086', 'VAR073', 'VAR013',\n",
       "       'VAR021', 'VAR223', 'var178', 'VAR014', 'VAR002', 'VAR054', 'VAR158',\n",
       "       'VAR081', 'VAR090', 'VAR109', 'VAR162', 'VAR144', 'VAR191', 'VAR155',\n",
       "       'VAR069', 'VAR195'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rst = pd.DataFrame(result.pvalues)\n",
    "rst.sort_values(by=[0], ascending=False).head(100).index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "#p값이 너무 높은 변수 제거하기\n",
    "df2 = df1.iloc[:,1:227]\n",
    "df4 = df2.drop(['VAR002','VAR005','VAR008','VAR012','VAR020'], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#x_train_all, x_test, y_train_all, y_test = train_test_split(df2, df1['MRC_ID_DI'], test_size=0.2, random_state=77,stratify=df1['MRC_ID_DI'])\n",
    "x_train_all, x_test, y_train_all, y_test = train_test_split(df4, df1['label'], test_size=0.2, random_state=77,stratify=df1['label'])\n",
    "\n",
    "#validation dataset 분류  x , X_train_all\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train_all, y_train_all, test_size=0.22, random_state=77, stratify=y_train_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 442, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 442, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 442, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 442, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 442, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 442, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 442, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 100.0, 'penalty': 'l2', 'random_state': 77}\n",
      "0.8494557864036804\n",
      "로지스틱 회귀 훈련 정확도: 0.869\n",
      "로지스틱 회귀 검증 정확도: 0.857\n",
      "로지스틱 회귀 테스트 정확도: 0.854\n",
      "로지스틱 회귀 테스트 f1: 0.861\n",
      "Predicted     0    1\n",
      "Actual              \n",
      "0          1533  107\n",
      "1           188  197\n"
     ]
    }
   ],
   "source": [
    "\n",
    "param_grid = {\n",
    "    \"C\":np.logspace(-3,3,7),\n",
    "    'penalty': ['l1','l2'],\n",
    "    'random_state' : [77] \n",
    "}\n",
    "\n",
    "clf = LogisticRegression(random_state=77)\n",
    "clf = GridSearchCV(clf, param_grid=param_grid) \n",
    "clf.fit(x_train, y_train)\n",
    "print(clf.best_params_)\n",
    "print(clf.best_score_)\n",
    "y_predt = clf.predict(x_train)\n",
    "print(\"로지스틱 회귀 훈련 정확도: {:.3f}\".format(accuracy_score(y_predt, y_train)))\n",
    "y_pred = clf.predict(x_val)\n",
    "print(\"로지스틱 회귀 검증 정확도: {:.3f}\".format(accuracy_score(y_pred, y_val)))\n",
    "y_pred = clf.predict(x_test)\n",
    "print(\"로지스틱 회귀 테스트 정확도: {:.3f}\".format(accuracy_score(y_pred, y_test)))\n",
    "print(\"로지스틱 회귀 테스트 f1: {:.3f}\".format(f1_score(y_pred, y_test, average='weighted')))\n",
    "\n",
    "confusion_matrix = pd.crosstab(y_test, y_pred, rownames=['Actual'], colnames=['Predicted'])\n",
    "print (confusion_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
